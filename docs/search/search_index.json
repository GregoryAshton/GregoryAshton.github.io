{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Dr Gregory Ashton About me I am a Lecturer in Physics at Royal Holloway, University of London . I began my research career at the University of Southampton working with Ian Jones and Reinhard Prix where I completed by PhD in 2016 on Timing variations in neutron stars: models, inference and their implications for gravitational waves . Following this I held a postdoc position at the Albert Einstein Institute, Hannover before moving in 2018 to Monash University in Australia to work as an Assistant Lecturer with Paul Lasky . I then briefly joined the Institute for Cosmology and Gravitation (ICG) at the University of Portsmouth working with Laura Nuttall as a Research Fellow before taking up my current post. My research interest is the relativistic astrophysics of neutron stars and black holes. I am a member of the LIGO Scientific Collaboration and co-chair the Collaborations largest and most active observational science group, the Compact Binary Group . Contact: gregory.ashton@ligo.org My CV ADS library News Today I gave a colloqium at the Institute for Cosmology and Gravitation, University of Portsmouth. I talked about Glitch-robust gravitational-wave astronomy! Double new paper day! Jack Heinzel submitted Inferring the Astrophysical Population of Gravitational Wave Sources in the Presence of Noise Transients to MNRAS for review while Rowina Nathan submitted Improving pulsar-timing solutions through dynamic pulse fitting also to MNRAS for review. Both of these are excellent PhD students I have had the good forture to help with their project. 06/Feb/2023 My paper Gaussian Processes for Glitch-robust Gravitational-wave Astronomy was published in MNRAS. 06/Dec/2022 I was invited to speak on Key results from ground-based gravitational-wave detectors at the GWPAW 2022 meeting. 01/Sep/2022 PhD student Mattia Emma started in my group! 13/Jul/2022 Our paper Parameterised population models of transient non-Gaussian noise in the LIGO gravitational-wave detectors has been published in CQG. 11/Jul/2022 I led the organisation of the Gravitational-wave astronomy parallel session at NAM2022 . We where over subscribed by a factor of 2 on talks and the room was full too. Really exciting to see the breadth and talent of people working on GWs in the UK. Thanks to all the other organisers who helped make it happen. 04/Jul/2022 My paper with Tim Dietrich The use of hypermodels to understand binary neutron star collisions was published in Nature Astronomy. 01/Jun/2022 Our Nested Sampling primer was published in Nature Reviews . While I am first author, you should realise this is because \"A\" is the first letter of the alphabet! Andrew Fowlie led the effort and I am deeply indepted to him as I learned a lot. My contribution, along with Matt Pitkin and John Veitch was to the gravitational-wave application section. 25/May/2022 Today, I joined the smallpeice trust and RHUL's Girl's Into Astrophysics event! 29/Apr/2022 It is a new paper day! GWCloud hit the arXiv which details the inner workings of searchable repository for the creation and curation of gravitational-wave inference results. This project started back in 2018 IIRC with several ADACS applications by Paul Lasky. It has been a pleasure to work on and I look forward to the future of the project. 27/Apr/2022 Today I had the pleasure of examing a PhD student thesis. I'll delay in giving the name until it is all official. But, they defended very well and can now proudly call themselves Docter! 06/Apr/2022 I was a judge for the best Student Prize at BritGrav 2022 . The conference consisted of two days of talks from students/postdocs and served as a great display of the exciting science done by scientists in the UK. The quality of talks was fantastic and it was hard to pick between them, but in the end Lucy Thomas won the best talk prize with Marion Cromb and Elsa Teixeira as runners up. Congratulations to them all. 01/Feb/2022 I am an Award Lead for the Alan Turing Network Development Award . I'm looking forward to using this to develop some interdisciplinary projects. 01/Dec/2021 Today I joined the SEPNet workshop Equality, Diversity & Inclusion \u2013 Revisiting the leaky pipeline \u2013 short-term contracts and career planning . You can find my slides here . 18/Nov/2021 I presented at the Banff IRS workshop Detection and Analysis of Gravitational Waves in the era of Multi-Messenger Astronomy: From Mathematical Modelling to Machine Learning . You can find a recording here and my slides here . 17/Nov/2021 With Tim Dietrich, we put out a new preprint Understanding binary neutron star collisions with hypermodels . This one shows some tentative evidence for waveform systematics in a BNS. 01/Nov/2021 Today I joined Royal Holloway as a Lecturer in Physics! 26/Oct/2021 A paper led by the ICG's Simone Mozzon Does non-stationary noise in LIGO and Virgo affect the estimation of H0? hit the arXiv today. 07/Oct/2021 New paper day! Parameterised population models of transient non-Gaussian noise in the LIGO gravitational-wave detectors . This work has taken nearly 2 years from the initial conception, a huge amount of computing, and lots of thinking. I really enjoyed getting into a new aspect of GW astronomy, namely the characterisation of the detector. 17/Sep/2021 My daughter Ada was born at 4am. Very happy, excited, and tired! 29/Jul/2021 New paper led by PhD student Avi Vajpeyi on A search for intermediate-mass black holes mergers in the second LIGO--Virgo observing run with the Bayes Coherence Ratio 14/Jul/2021 Today I helped out in Royal Holloway's Girls into Physics program run by the Small Piece trust where we did Python programming for science 29/Jun/2021 The LVK collaboration published Observation of gravitational waves from two neutron star-black hole coalescences in ApJL. This was a really fun project to be involved in. That we can make a definitive statement about the nature of the secondary (light-mass) object with a counterpart is testament to our understanding of controlling systematic uncertainty. 17/Jun/2021 We released a new paper: Bilby-MCMC: An MCMC sampler for gravitational-wave inference . This was really fun as I got to write a sampler from scratch and remind myself of all the gory details. 16/Jun/2021 I helped out with the Royal Holloway Particle Physics Masterclass introducing python programming. 11/May/2021 New paper with PhD student Zhi-Qiang You: Optimized localization for gravitational-waves from merging binaries submitted to MNRAS. 10/May/2021 The Gravitational Wave Open Data Workshop #4 kicked off today on gather.town . 12/Apr/2021 Today I was elected as co-chair of the LIGO collaboration Compact Binary Coalescence (CBC) group. I join Chad Hanna and Walter Del Pozzo and look forward to helping coordinate the discovery and analysis of signals from colliding black holes and neutron stars. 08/Apr/2021 Today I'm taking part in Royal Holloway's Astrophysics Residential for 2021. I'm covering an introduction to programming and how to calculate Pi using random numbers . 12/Mar/2021 My students successfully nominated me for a You\u2019re Valued Award at Royal Holloway . I'm proud to add some of their comments here: \u201cGreg is an amazing teacher. His enthusiastic approach to teaching maths to us foundation year students has made us enthusiastic as well. he is very friendly and approachable and when we need help he very patiently helps explain everything to us. He is dedicated to make us understand all the concepts he teaches and help us learn to our best capacity.\u201d and \u201cGreg always focuses on our individual worries and makes sure we understand before moving on. He's very good at teaching and we're happy to have him as our maths professor :)\u201d. 08/Feb/2021 I presented Flickering of the Vela pulsar to the CAMK journal club. 28/Jan/2021 My PhD student Nikhil Sarin passed his pre-submission milestone. It has been a pleasure to watch Nik develop into a great researcher - his next employer will be lucky to have him. 27/Jan/2021 New paper on arXiv! Work led by David Keitel \"PyFstat: a Python package for continuous gravitational-wave data analysis\" which updates the latest on the PyFstat package for Continuous-Wave analyses. 14/Jan/2021 New paper! Work led by Eric Burns \"Identification of a Local Sample of Gamma-Ray Bursts Consistent with a Magnetar Giant Flare Origin\" hits the arXiv. 11/Jan/2021 Semester 2 gets underway at RHUL. I'm teaching FY1006 Mathematics II to 140 students in online mode for the foreseable future. 07/Jan/2021 I gave a talk titled \"The deepening mystery of the Vela radio-pulsar glitch\" at the Department of Physics, Bar-Ilan University on our recent Vela paper. 04/Dec/2020 I gave an improptu talk to the University of Southampton's astrophysics group on our recent Vela paper. 16/Nov/2020 New paper \"Flickering of the Vela pulsar during its 2016 glitch\" on the arXiv. 10/Nov/2020 I was awarded the 2020 USERN Physical and Chemical Sciences prize . 04/Nov/2020 I gave a presentation to Royal Holloway's Physics group \"Turning Wiggles into Science\" Gallery","title":"Home"},{"location":"#dr-gregory-ashton","text":"","title":"Dr Gregory Ashton"},{"location":"#about-me","text":"I am a Lecturer in Physics at Royal Holloway, University of London . I began my research career at the University of Southampton working with Ian Jones and Reinhard Prix where I completed by PhD in 2016 on Timing variations in neutron stars: models, inference and their implications for gravitational waves . Following this I held a postdoc position at the Albert Einstein Institute, Hannover before moving in 2018 to Monash University in Australia to work as an Assistant Lecturer with Paul Lasky . I then briefly joined the Institute for Cosmology and Gravitation (ICG) at the University of Portsmouth working with Laura Nuttall as a Research Fellow before taking up my current post. My research interest is the relativistic astrophysics of neutron stars and black holes. I am a member of the LIGO Scientific Collaboration and co-chair the Collaborations largest and most active observational science group, the Compact Binary Group . Contact: gregory.ashton@ligo.org My CV ADS library","title":"About me"},{"location":"#news","text":"Today I gave a colloqium at the Institute for Cosmology and Gravitation, University of Portsmouth. I talked about Glitch-robust gravitational-wave astronomy! Double new paper day! Jack Heinzel submitted Inferring the Astrophysical Population of Gravitational Wave Sources in the Presence of Noise Transients to MNRAS for review while Rowina Nathan submitted Improving pulsar-timing solutions through dynamic pulse fitting also to MNRAS for review. Both of these are excellent PhD students I have had the good forture to help with their project. 06/Feb/2023 My paper Gaussian Processes for Glitch-robust Gravitational-wave Astronomy was published in MNRAS. 06/Dec/2022 I was invited to speak on Key results from ground-based gravitational-wave detectors at the GWPAW 2022 meeting. 01/Sep/2022 PhD student Mattia Emma started in my group! 13/Jul/2022 Our paper Parameterised population models of transient non-Gaussian noise in the LIGO gravitational-wave detectors has been published in CQG. 11/Jul/2022 I led the organisation of the Gravitational-wave astronomy parallel session at NAM2022 . We where over subscribed by a factor of 2 on talks and the room was full too. Really exciting to see the breadth and talent of people working on GWs in the UK. Thanks to all the other organisers who helped make it happen. 04/Jul/2022 My paper with Tim Dietrich The use of hypermodels to understand binary neutron star collisions was published in Nature Astronomy. 01/Jun/2022 Our Nested Sampling primer was published in Nature Reviews . While I am first author, you should realise this is because \"A\" is the first letter of the alphabet! Andrew Fowlie led the effort and I am deeply indepted to him as I learned a lot. My contribution, along with Matt Pitkin and John Veitch was to the gravitational-wave application section. 25/May/2022 Today, I joined the smallpeice trust and RHUL's Girl's Into Astrophysics event! 29/Apr/2022 It is a new paper day! GWCloud hit the arXiv which details the inner workings of searchable repository for the creation and curation of gravitational-wave inference results. This project started back in 2018 IIRC with several ADACS applications by Paul Lasky. It has been a pleasure to work on and I look forward to the future of the project. 27/Apr/2022 Today I had the pleasure of examing a PhD student thesis. I'll delay in giving the name until it is all official. But, they defended very well and can now proudly call themselves Docter! 06/Apr/2022 I was a judge for the best Student Prize at BritGrav 2022 . The conference consisted of two days of talks from students/postdocs and served as a great display of the exciting science done by scientists in the UK. The quality of talks was fantastic and it was hard to pick between them, but in the end Lucy Thomas won the best talk prize with Marion Cromb and Elsa Teixeira as runners up. Congratulations to them all. 01/Feb/2022 I am an Award Lead for the Alan Turing Network Development Award . I'm looking forward to using this to develop some interdisciplinary projects. 01/Dec/2021 Today I joined the SEPNet workshop Equality, Diversity & Inclusion \u2013 Revisiting the leaky pipeline \u2013 short-term contracts and career planning . You can find my slides here . 18/Nov/2021 I presented at the Banff IRS workshop Detection and Analysis of Gravitational Waves in the era of Multi-Messenger Astronomy: From Mathematical Modelling to Machine Learning . You can find a recording here and my slides here . 17/Nov/2021 With Tim Dietrich, we put out a new preprint Understanding binary neutron star collisions with hypermodels . This one shows some tentative evidence for waveform systematics in a BNS. 01/Nov/2021 Today I joined Royal Holloway as a Lecturer in Physics! 26/Oct/2021 A paper led by the ICG's Simone Mozzon Does non-stationary noise in LIGO and Virgo affect the estimation of H0? hit the arXiv today. 07/Oct/2021 New paper day! Parameterised population models of transient non-Gaussian noise in the LIGO gravitational-wave detectors . This work has taken nearly 2 years from the initial conception, a huge amount of computing, and lots of thinking. I really enjoyed getting into a new aspect of GW astronomy, namely the characterisation of the detector. 17/Sep/2021 My daughter Ada was born at 4am. Very happy, excited, and tired! 29/Jul/2021 New paper led by PhD student Avi Vajpeyi on A search for intermediate-mass black holes mergers in the second LIGO--Virgo observing run with the Bayes Coherence Ratio 14/Jul/2021 Today I helped out in Royal Holloway's Girls into Physics program run by the Small Piece trust where we did Python programming for science 29/Jun/2021 The LVK collaboration published Observation of gravitational waves from two neutron star-black hole coalescences in ApJL. This was a really fun project to be involved in. That we can make a definitive statement about the nature of the secondary (light-mass) object with a counterpart is testament to our understanding of controlling systematic uncertainty. 17/Jun/2021 We released a new paper: Bilby-MCMC: An MCMC sampler for gravitational-wave inference . This was really fun as I got to write a sampler from scratch and remind myself of all the gory details. 16/Jun/2021 I helped out with the Royal Holloway Particle Physics Masterclass introducing python programming. 11/May/2021 New paper with PhD student Zhi-Qiang You: Optimized localization for gravitational-waves from merging binaries submitted to MNRAS. 10/May/2021 The Gravitational Wave Open Data Workshop #4 kicked off today on gather.town . 12/Apr/2021 Today I was elected as co-chair of the LIGO collaboration Compact Binary Coalescence (CBC) group. I join Chad Hanna and Walter Del Pozzo and look forward to helping coordinate the discovery and analysis of signals from colliding black holes and neutron stars. 08/Apr/2021 Today I'm taking part in Royal Holloway's Astrophysics Residential for 2021. I'm covering an introduction to programming and how to calculate Pi using random numbers . 12/Mar/2021 My students successfully nominated me for a You\u2019re Valued Award at Royal Holloway . I'm proud to add some of their comments here: \u201cGreg is an amazing teacher. His enthusiastic approach to teaching maths to us foundation year students has made us enthusiastic as well. he is very friendly and approachable and when we need help he very patiently helps explain everything to us. He is dedicated to make us understand all the concepts he teaches and help us learn to our best capacity.\u201d and \u201cGreg always focuses on our individual worries and makes sure we understand before moving on. He's very good at teaching and we're happy to have him as our maths professor :)\u201d. 08/Feb/2021 I presented Flickering of the Vela pulsar to the CAMK journal club. 28/Jan/2021 My PhD student Nikhil Sarin passed his pre-submission milestone. It has been a pleasure to watch Nik develop into a great researcher - his next employer will be lucky to have him. 27/Jan/2021 New paper on arXiv! Work led by David Keitel \"PyFstat: a Python package for continuous gravitational-wave data analysis\" which updates the latest on the PyFstat package for Continuous-Wave analyses. 14/Jan/2021 New paper! Work led by Eric Burns \"Identification of a Local Sample of Gamma-Ray Bursts Consistent with a Magnetar Giant Flare Origin\" hits the arXiv. 11/Jan/2021 Semester 2 gets underway at RHUL. I'm teaching FY1006 Mathematics II to 140 students in online mode for the foreseable future. 07/Jan/2021 I gave a talk titled \"The deepening mystery of the Vela radio-pulsar glitch\" at the Department of Physics, Bar-Ilan University on our recent Vela paper. 04/Dec/2020 I gave an improptu talk to the University of Southampton's astrophysics group on our recent Vela paper. 16/Nov/2020 New paper \"Flickering of the Vela pulsar during its 2016 glitch\" on the arXiv. 10/Nov/2020 I was awarded the 2020 USERN Physical and Chemical Sciences prize . 04/Nov/2020 I gave a presentation to Royal Holloway's Physics group \"Turning Wiggles into Science\"","title":"News"},{"location":"#gallery","text":"","title":"Gallery"},{"location":"help/","text":"Help This page contains some helpful hints and tips that I've leared through my career. I don't claim these to be perfect solutions, you have suggestions, please feel free to email me. Writing a paper/document Writing scientific papers is a central part of life as a scientist. Like most things, investing a bit of time/money can make the process a little easier and allow you to focus on writing rather than the boring admin that inevitably goes with it. Here are a few ad-hoc things I've picked up which may help: Choose your medium . There are many ways to write a paper. You can use latex directly on your computer (i.e. write a .tex file and compile it with pdflatex ); you can use cloud-based latex solutions like Overleaf (see below); or you could use Word/Google Docs. All of these are great and have their place. Personally, I use latex directly when I'm working on a paper with more than 10 contributors and we need to use git to track changes and merges. I use Overleaf for most papers I write, and I use Word/Google Docs for pretty much anything which does not required images or equations. Use the tools . There are a number of tools which can simplify your life. Taking the pain out of compilling latex, to automatically identifying and fixing spelling, gramatical, and typographic errors. As someone who has some mild dyslexia (and did not pay enough attention at school), I've found these invaluable. Given the availability of the checkers (most are free), it demonstrates real laziness to submit journal articles with typos (something I have definitely done). Here are my favourits: Overleaf : this is a cloud-based latex editor/compiler. For projects where you have a handful of collaborators, the free version works great! Make sure to use the built-in spelling correction. Overlead has an incredible ability to ignore Latex errors and still produce output. This is really useful during the the writing stage. But please, for the love of all that is good in this world, fix those errors eventually . They will result in broken arXiv compillations and usually many some aspect of your document is missing. Grammarly : there is a paid version which provides more in-depthanalysis. In my mind this is worth every penny if you are submitting grants/fellowships where a single typo could upset a reviewer (they are fickle beasts). But, the free version is also great. You can use this for Email, Word documents, and Overleaf (see this S/O page ). codespell : this is great for when you are forced to work on a latex document without overleaf. Use latex macros for results : If your paper contains results from any sort of data analysis, it is likely you'll need to communicate some numbers (e.g. the mass of a star). Please, save yourself much heartache, do not hardcode those numbers into the paper. Instead, create a macro: \\newcommand{\\massofstar}{32.5} then anywhere you refer to the mass, you simply write \\massofstar . Better yet, have your analysis script write the newcommand lines for you in a file macros.tex , then add \\include{macros} in your latex document. Now, when you inevitably find a bug in your code the day before submission, you simply rerun and replace macos.tex . This also allows you to diff between the new and old macros.tex to see what changed! Finally, avoid editing the macros by hand. If you need to round the numbers, do this in the analysis script (this usually takes some thinking, but the uncertainties package can help here). Use latex macros for everything : Another common headache the day before submission is that you realise you have used \\theta to mean to different things. Doh! Or, perhaps your collaborator doesn't like you calling the dimensionaless spin of the black hole \\chi and prefers a_1 . To avoid the dangers of a last-minute Ctrl+F and replace, when you start writing never use symboles directly unless your are super super confident they will not change. Instead, define macros like \\newcommand{\\dimensionalspinmaginitude}{a_1} Being verbose is good here: it means you and your collaborators know that you mean the dimensionless spin magnitude, and not something else! You can also do this for things like software names where the formatting is usually journal specific. Add macros like \\newcommand{\\bilby}{\\textsc{Bilby}} Then when you realise the journal prefers texttt , you change one thing. Organise your bibtex files . Pick a convention for the keys (I like AUTHOR_YEAR , but oftentimes people like to use the ADS/Inspire key) and stick with it. This avoids repeated entries. You can use online tools like this to tidy up a messy bib file. Both NASA ADS and InspireHEP have nice utilisies to generate bibtex files (see, e.g. here and click cite here ). They also allow you to pick the format as well. Personally, I'm not a fan of maintaining a single huge bib file: I write a new one for each paper. Learn some grammar : Honestly, I could not of cared less about grammar while at school. Only during my final year as an undergraduate, when we had to write a 10-page report, I realised that actually, communicating what I had learned was maybe a little bit important. While Grammarly and other tools can help, it is good to learn why they are making changes. Equally, if you have a collaborator who edits the text, ask them why and don't be ashamed. Better to learn later than never. Be consistent in your style : English is a flexible language, but when reading a scientific article, a consistent style of writing avoids distracting the reader. For me, I found \"The Elements of Style\" by Strunk and White to be hugely useful. I don't agree with all of it, but I find it useful to have a single point of reference to consult when I'm unsure. I don't always follow the advice (often you need the passive voice), but I try to maintain it on the whole. There is also an automated checker in perl which you can use to identify problem areas. My script/job/test is not working, what should I do? This is a very common problem. For anyone involved in scientific research which relies on an element of programming, you will no doubt find that, at some point you are faced with a situation where \"it isn't doing what it should be!\" or \"it won't run!\". For those of us, like me, who did not receive formal training on software development, this can be daunting. Your supervisor or colleagues may expect you to solve it without help, what do you do? It might be tempting to either give up or ask for someone else to fix it. But, before you do either of things try to work through this list of steps: Breath : it can be frustrating so first things first, try to take a step back. Grab a cup of tea/coffee/water and let your head clear. Read : now, try reading both your script and the error message (in Python, this is called the TraceBack ). The output messages might be quite long, but try scanning through it and find: Which line of your script causes the error? What is it trying to do? What is the error message (if any) from the output? Does it tell you which program caused the issue? You may find this guide useful in understanding the TraceBack Check : Now check out what else is going on: How long does it take to fail, does it take a long time to fail or straight away? What else is happening with your computer at the time? Does the memory/CPU usage of the program spike? If your script relies on software, what version are you using? Is it up to date, did you update it recently? Have you backed up your script, can you roll back to an earlier version to see if it works? Investigate: hopefully by reading the scripts and logs, you might have an inkling about what is going on. Try to test it out, perhaps remove the line that is failing. What happens? Do you get a new error message? The goal here is to build some intuition about what the error is and the cause. Research: Now that you have an idea about what is going on, it is time to research. A few places that are worth searching: Google/StackOverflow : An easy first step is to simple search the error message on the internet. But, be warned: copy and pasting solutions is usually a recipe for disaster. Instead, read the solutions and comments to help you understand how they solve the problem. Documentation : If the problem seems to be in a specific piece of software, try to find the documentation and scan it. Do they have a guide on troubleshooting, a place where they welcome questions? Source code : If you have identified which line is failing in any underlying software, find the source code (if it is open source). Read it over. It may well be that you have found a bug: this is a great oppotunity to contribute. If you can see how to fix it, try editing the source code, this may require you to install the software from the source code to test your fix. If your fix is successful, consider opening a Merge Request/Pull Request to fix the code. Identify who you will ask for help : If you have run through the steps above and things are still not working, you are now ready to ask for help. First, you need to identify who you will ask If the problem is in a specific piece of software, does that software have a guide for getting help? If you are working with a colleague/supervisor, they may be the right persont to contact now. If your questions are more general, try StackOverflow or look for local programming support groups (e.g. at your University). Ask for help: Now you know who to contact, you should write a message to them. Keep in mind the following: Keep it concise. Avoid long paragraphs and lots of background, focus on what they need to know to help you. Include a Minimum Working Example . This will help them reproduce your problem. In making it you may even solve the problem yourself! Include a description of your investigation and research: they will need to know all the details (i.e. versions of software, any oddities) Tell them what you have tried already. This will make sure they don't repeat the same investigations you have undertaken and help them to help you. If relevant, tell them why you are contacting them. If they have offered support, thank them for it! In some cases, what may appear to be a \"missing feature\" is in fact a large research project. If you need this feature, you may consider offering them an acknowledgement or even co-authorship on any subsequent publications. After all of this, send your message. Have some patience, they may be overwhelmed by other tasks when you email. Feel free to email them again after a week or so if you haven't got an answer. They may have simply forgotten! Getting started in the LIGO Collaboration There is a guide to getting started in the Collaboration, you can find this here . This contains a wealth of information. Here, I'm just listing a few other things I find useful. If you use the Google chrome browser, you can use a Profile associated with your LIGO account. This creates a separate browser window which, by default, logs in to things with your LIGO credentials. This greatly reduced the amount of clicking between different Google logins I have to do when following links. Want to email someone in the collaboration, but not sure of their address? You can head to https://contacts.google.com/ and log in with your LIGO Credentials. This will provide you with all the albert.einstein@ligo.org addresses. With this, you can download it as a CSV and upload it to your email client. Now you have all of LIGO on tab completion! Getting a usable linux environment on Windows The Windows OS used to be unusable for most scientists due to a host of compatibility issues. But, Windows now offers the \"Windows Subsystem for Linux\" which has changed that. Here are some links to useful guides Installing Ubuntu in WSL . The default OS is Ubuntu, this has worked well for me. It is worth making sure you install WSL2. In theory, WSL now supports GUI apps natively (see here ). I have yet to get this working, but it looks great. I have found that the Windows Terminal works really well. I actually prefer this to the native terminal in Ubuntu itself. There is a SO post here on getting it to launch WSL, but I think this may be a little out of date. You can add WSL by simply adding a new profile. Getting a usable python environment (in WSL or in Linux) I strongly recommend people use anaconda (AKA conda) to manage their python installation. The full instructions can be found here . If you are working in Ubuntu through WSL, follow the instructions for Linux/Debian . I'll provide a quick TLDR here: Install the prerequisites Scroll to the bottom of this page and copy the link address for the Linux install, something like \"64-Bit (x86) Installer (581 MB)\" i.e. right click \"Download\" and hit Copy link address) Run wget <THE LINK ADDRESS to download the file (Note: you can just download it from the browser, but if you are working in a virtual machine/on a cluster/in WSL you'll then need to move the file from your local machine to the target. Using wget directly skips this step). Install anaconda by running bash ~/PATH/TO/FILE where the FILE will be something like Anaconda3-2020.02-Linux-x86_64.sh Finally, once you have installed conda and restarted, your command prompt will have start with (base) . This tells you that you are in the base conda environment. DO NOT INSTALL THINGS IN THE BASE ENVIRONMENT. Instead, follow this guide to create a new environment. For example, to create an environment called testing with python 3.9 , run conda create -n testing python=3.9 . Once complete, you'll need to activate the environment conda activate testing . While conda may seem like a headache (you will no doubt forget to activate the environment occasionally and wonder why nothing works!). This headache is minor compared to the headache which occurs when you need to install two different versions of numpy for two different projects. As such, use a new environment for each project you work on. Using git to track initial project progress git is a super powerful tool. Most people have heard of github or gitlab , these are websites that store git repositories and allow people to collaborate in building software, to provide easy access, and to track developments in a meaningful way. However, git itself is simply a version control software and can be used without pushing the repository to github or gitlab . Here I describe a simple way I use git to track progress of projects during the initial conception. Okay, so you have a cool idea for a project and you spent a day writing a hacky script which is the proof of concept. Great, now you want to expand it out, maybe add some bells and whistles. Before you do anything else. Put the script + anything else into a directory with a half sensible name, then run git init , git add * , and git commit -m \"Initial commit adding everything\" . Now you have a snapshot of the code. Now continue as you would anyway, perhaps adding a first bell then a whistle. But oh no! You broke the proof of concept... but how? Let's figure it out.. run git diff and you'll see the changes against your last working version. You spot the type fix it and then again add and commit everything. Continue this process. The more often you stop and commit things the better. At some point either you will figure out the project is a donut, in which case put it in the failed projects directory and move on. Or, you think it has legs. At this point you may choose to start a \"fresh\" git repo and try to make your commits more serious (no more curse words when you break it!). Or, you could just push your current repo to github. Who knows, when you win the Nobel prize for your ieda people may look back at those early commits to spot your genious! I think too often we wait far to long to start using proper project management tools. By allowing yourself to just use git locally (i.e. you don't need to push to a repo, set up a CI, and add testing), we gain a lot of benefits without the overhead.","title":"Help"},{"location":"help/#help","text":"This page contains some helpful hints and tips that I've leared through my career. I don't claim these to be perfect solutions, you have suggestions, please feel free to email me.","title":"Help"},{"location":"help/#writing-a-paperdocument","text":"Writing scientific papers is a central part of life as a scientist. Like most things, investing a bit of time/money can make the process a little easier and allow you to focus on writing rather than the boring admin that inevitably goes with it. Here are a few ad-hoc things I've picked up which may help: Choose your medium . There are many ways to write a paper. You can use latex directly on your computer (i.e. write a .tex file and compile it with pdflatex ); you can use cloud-based latex solutions like Overleaf (see below); or you could use Word/Google Docs. All of these are great and have their place. Personally, I use latex directly when I'm working on a paper with more than 10 contributors and we need to use git to track changes and merges. I use Overleaf for most papers I write, and I use Word/Google Docs for pretty much anything which does not required images or equations. Use the tools . There are a number of tools which can simplify your life. Taking the pain out of compilling latex, to automatically identifying and fixing spelling, gramatical, and typographic errors. As someone who has some mild dyslexia (and did not pay enough attention at school), I've found these invaluable. Given the availability of the checkers (most are free), it demonstrates real laziness to submit journal articles with typos (something I have definitely done). Here are my favourits: Overleaf : this is a cloud-based latex editor/compiler. For projects where you have a handful of collaborators, the free version works great! Make sure to use the built-in spelling correction. Overlead has an incredible ability to ignore Latex errors and still produce output. This is really useful during the the writing stage. But please, for the love of all that is good in this world, fix those errors eventually . They will result in broken arXiv compillations and usually many some aspect of your document is missing. Grammarly : there is a paid version which provides more in-depthanalysis. In my mind this is worth every penny if you are submitting grants/fellowships where a single typo could upset a reviewer (they are fickle beasts). But, the free version is also great. You can use this for Email, Word documents, and Overleaf (see this S/O page ). codespell : this is great for when you are forced to work on a latex document without overleaf. Use latex macros for results : If your paper contains results from any sort of data analysis, it is likely you'll need to communicate some numbers (e.g. the mass of a star). Please, save yourself much heartache, do not hardcode those numbers into the paper. Instead, create a macro: \\newcommand{\\massofstar}{32.5} then anywhere you refer to the mass, you simply write \\massofstar . Better yet, have your analysis script write the newcommand lines for you in a file macros.tex , then add \\include{macros} in your latex document. Now, when you inevitably find a bug in your code the day before submission, you simply rerun and replace macos.tex . This also allows you to diff between the new and old macros.tex to see what changed! Finally, avoid editing the macros by hand. If you need to round the numbers, do this in the analysis script (this usually takes some thinking, but the uncertainties package can help here). Use latex macros for everything : Another common headache the day before submission is that you realise you have used \\theta to mean to different things. Doh! Or, perhaps your collaborator doesn't like you calling the dimensionaless spin of the black hole \\chi and prefers a_1 . To avoid the dangers of a last-minute Ctrl+F and replace, when you start writing never use symboles directly unless your are super super confident they will not change. Instead, define macros like \\newcommand{\\dimensionalspinmaginitude}{a_1} Being verbose is good here: it means you and your collaborators know that you mean the dimensionless spin magnitude, and not something else! You can also do this for things like software names where the formatting is usually journal specific. Add macros like \\newcommand{\\bilby}{\\textsc{Bilby}} Then when you realise the journal prefers texttt , you change one thing. Organise your bibtex files . Pick a convention for the keys (I like AUTHOR_YEAR , but oftentimes people like to use the ADS/Inspire key) and stick with it. This avoids repeated entries. You can use online tools like this to tidy up a messy bib file. Both NASA ADS and InspireHEP have nice utilisies to generate bibtex files (see, e.g. here and click cite here ). They also allow you to pick the format as well. Personally, I'm not a fan of maintaining a single huge bib file: I write a new one for each paper. Learn some grammar : Honestly, I could not of cared less about grammar while at school. Only during my final year as an undergraduate, when we had to write a 10-page report, I realised that actually, communicating what I had learned was maybe a little bit important. While Grammarly and other tools can help, it is good to learn why they are making changes. Equally, if you have a collaborator who edits the text, ask them why and don't be ashamed. Better to learn later than never. Be consistent in your style : English is a flexible language, but when reading a scientific article, a consistent style of writing avoids distracting the reader. For me, I found \"The Elements of Style\" by Strunk and White to be hugely useful. I don't agree with all of it, but I find it useful to have a single point of reference to consult when I'm unsure. I don't always follow the advice (often you need the passive voice), but I try to maintain it on the whole. There is also an automated checker in perl which you can use to identify problem areas.","title":"Writing a paper/document"},{"location":"help/#my-scriptjobtest-is-not-working-what-should-i-do","text":"This is a very common problem. For anyone involved in scientific research which relies on an element of programming, you will no doubt find that, at some point you are faced with a situation where \"it isn't doing what it should be!\" or \"it won't run!\". For those of us, like me, who did not receive formal training on software development, this can be daunting. Your supervisor or colleagues may expect you to solve it without help, what do you do? It might be tempting to either give up or ask for someone else to fix it. But, before you do either of things try to work through this list of steps: Breath : it can be frustrating so first things first, try to take a step back. Grab a cup of tea/coffee/water and let your head clear. Read : now, try reading both your script and the error message (in Python, this is called the TraceBack ). The output messages might be quite long, but try scanning through it and find: Which line of your script causes the error? What is it trying to do? What is the error message (if any) from the output? Does it tell you which program caused the issue? You may find this guide useful in understanding the TraceBack Check : Now check out what else is going on: How long does it take to fail, does it take a long time to fail or straight away? What else is happening with your computer at the time? Does the memory/CPU usage of the program spike? If your script relies on software, what version are you using? Is it up to date, did you update it recently? Have you backed up your script, can you roll back to an earlier version to see if it works? Investigate: hopefully by reading the scripts and logs, you might have an inkling about what is going on. Try to test it out, perhaps remove the line that is failing. What happens? Do you get a new error message? The goal here is to build some intuition about what the error is and the cause. Research: Now that you have an idea about what is going on, it is time to research. A few places that are worth searching: Google/StackOverflow : An easy first step is to simple search the error message on the internet. But, be warned: copy and pasting solutions is usually a recipe for disaster. Instead, read the solutions and comments to help you understand how they solve the problem. Documentation : If the problem seems to be in a specific piece of software, try to find the documentation and scan it. Do they have a guide on troubleshooting, a place where they welcome questions? Source code : If you have identified which line is failing in any underlying software, find the source code (if it is open source). Read it over. It may well be that you have found a bug: this is a great oppotunity to contribute. If you can see how to fix it, try editing the source code, this may require you to install the software from the source code to test your fix. If your fix is successful, consider opening a Merge Request/Pull Request to fix the code. Identify who you will ask for help : If you have run through the steps above and things are still not working, you are now ready to ask for help. First, you need to identify who you will ask If the problem is in a specific piece of software, does that software have a guide for getting help? If you are working with a colleague/supervisor, they may be the right persont to contact now. If your questions are more general, try StackOverflow or look for local programming support groups (e.g. at your University). Ask for help: Now you know who to contact, you should write a message to them. Keep in mind the following: Keep it concise. Avoid long paragraphs and lots of background, focus on what they need to know to help you. Include a Minimum Working Example . This will help them reproduce your problem. In making it you may even solve the problem yourself! Include a description of your investigation and research: they will need to know all the details (i.e. versions of software, any oddities) Tell them what you have tried already. This will make sure they don't repeat the same investigations you have undertaken and help them to help you. If relevant, tell them why you are contacting them. If they have offered support, thank them for it! In some cases, what may appear to be a \"missing feature\" is in fact a large research project. If you need this feature, you may consider offering them an acknowledgement or even co-authorship on any subsequent publications. After all of this, send your message. Have some patience, they may be overwhelmed by other tasks when you email. Feel free to email them again after a week or so if you haven't got an answer. They may have simply forgotten!","title":"My script/job/test is not working, what should I do?"},{"location":"help/#getting-started-in-the-ligo-collaboration","text":"There is a guide to getting started in the Collaboration, you can find this here . This contains a wealth of information. Here, I'm just listing a few other things I find useful. If you use the Google chrome browser, you can use a Profile associated with your LIGO account. This creates a separate browser window which, by default, logs in to things with your LIGO credentials. This greatly reduced the amount of clicking between different Google logins I have to do when following links. Want to email someone in the collaboration, but not sure of their address? You can head to https://contacts.google.com/ and log in with your LIGO Credentials. This will provide you with all the albert.einstein@ligo.org addresses. With this, you can download it as a CSV and upload it to your email client. Now you have all of LIGO on tab completion!","title":"Getting started in the LIGO Collaboration"},{"location":"help/#getting-a-usable-linux-environment-on-windows","text":"The Windows OS used to be unusable for most scientists due to a host of compatibility issues. But, Windows now offers the \"Windows Subsystem for Linux\" which has changed that. Here are some links to useful guides Installing Ubuntu in WSL . The default OS is Ubuntu, this has worked well for me. It is worth making sure you install WSL2. In theory, WSL now supports GUI apps natively (see here ). I have yet to get this working, but it looks great. I have found that the Windows Terminal works really well. I actually prefer this to the native terminal in Ubuntu itself. There is a SO post here on getting it to launch WSL, but I think this may be a little out of date. You can add WSL by simply adding a new profile.","title":"Getting a usable linux environment on Windows"},{"location":"help/#getting-a-usable-python-environment-in-wsl-or-in-linux","text":"I strongly recommend people use anaconda (AKA conda) to manage their python installation. The full instructions can be found here . If you are working in Ubuntu through WSL, follow the instructions for Linux/Debian . I'll provide a quick TLDR here: Install the prerequisites Scroll to the bottom of this page and copy the link address for the Linux install, something like \"64-Bit (x86) Installer (581 MB)\" i.e. right click \"Download\" and hit Copy link address) Run wget <THE LINK ADDRESS to download the file (Note: you can just download it from the browser, but if you are working in a virtual machine/on a cluster/in WSL you'll then need to move the file from your local machine to the target. Using wget directly skips this step). Install anaconda by running bash ~/PATH/TO/FILE where the FILE will be something like Anaconda3-2020.02-Linux-x86_64.sh Finally, once you have installed conda and restarted, your command prompt will have start with (base) . This tells you that you are in the base conda environment. DO NOT INSTALL THINGS IN THE BASE ENVIRONMENT. Instead, follow this guide to create a new environment. For example, to create an environment called testing with python 3.9 , run conda create -n testing python=3.9 . Once complete, you'll need to activate the environment conda activate testing . While conda may seem like a headache (you will no doubt forget to activate the environment occasionally and wonder why nothing works!). This headache is minor compared to the headache which occurs when you need to install two different versions of numpy for two different projects. As such, use a new environment for each project you work on.","title":"Getting a usable python environment (in WSL or in Linux)"},{"location":"help/#using-git-to-track-initial-project-progress","text":"git is a super powerful tool. Most people have heard of github or gitlab , these are websites that store git repositories and allow people to collaborate in building software, to provide easy access, and to track developments in a meaningful way. However, git itself is simply a version control software and can be used without pushing the repository to github or gitlab . Here I describe a simple way I use git to track progress of projects during the initial conception. Okay, so you have a cool idea for a project and you spent a day writing a hacky script which is the proof of concept. Great, now you want to expand it out, maybe add some bells and whistles. Before you do anything else. Put the script + anything else into a directory with a half sensible name, then run git init , git add * , and git commit -m \"Initial commit adding everything\" . Now you have a snapshot of the code. Now continue as you would anyway, perhaps adding a first bell then a whistle. But oh no! You broke the proof of concept... but how? Let's figure it out.. run git diff and you'll see the changes against your last working version. You spot the type fix it and then again add and commit everything. Continue this process. The more often you stop and commit things the better. At some point either you will figure out the project is a donut, in which case put it in the failed projects directory and move on. Or, you think it has legs. At this point you may choose to start a \"fresh\" git repo and try to make your commits more serious (no more curse words when you break it!). Or, you could just push your current repo to github. Who knows, when you win the Nobel prize for your ieda people may look back at those early commits to spot your genious! I think too often we wait far to long to start using proper project management tools. By allowing yourself to just use git locally (i.e. you don't need to push to a repo, set up a CI, and add testing), we gain a lot of benefits without the overhead.","title":"Using git to track initial project progress"},{"location":"notes/","text":"Notes Projecting waveforms Derivations Drawing samples from the Goodman & Weare (2010) proposal distribution Old notes 11/Feb/20: Understanding PP plots 27/Mar/19: Importance Reweighting Example 08/Oct/19: Chirp time bounds","title":"Notes"},{"location":"notes/#notes","text":"Projecting waveforms","title":"Notes"},{"location":"notes/#derivations","text":"Drawing samples from the Goodman & Weare (2010) proposal distribution","title":"Derivations"},{"location":"notes/#old-notes","text":"11/Feb/20: Understanding PP plots 27/Mar/19: Importance Reweighting Example 08/Oct/19: Chirp time bounds","title":"Old notes"},{"location":"projects/","text":"Projects In this page, I collect various miscellaneous coding projects I have been involved in. Python for Science A set of workbooks to introduce python for scientific programming koala_html A package to quickly create html pages from collections of images kookaburra A python package for profile-domain timing of radio pulsars parallel_bilby A python package to leverage slurm-based HPC clusters. Enables scaling inference jobs up to many hundreds of cores. bilby_pipe A python package for automating the job of running multiple jobs on LIGO Data Grid clusters. bilby A python package providing a user friendly interface to perform parameter estimation. It is primarily designed and built for inference of compact binary coalescence events in interferometric data, but it can also be used for more general problems. PyFstat A python package containing various methods to run continuous gravitational wave searches. Includes glitch-robust, MCMC-based, and transient work. Bayes Bimodal Test A simple python module using the emcee MCMC software to perform a Bayesian model comparison of bimodality. GitCheck : A python appindicator which provides a visual check of the status of git repos. This builds on some of the functionality of batchgit by Max Hebditch. GetTrainTimes : a command-line tool to quickly get train times from the national rail (UK) website. pyweather : a command-line tool to quickly get a visual (ASCII) forecast of the weather for (almost) any location. Using the Google maps API to study average driving speeds around the globe . To see the results have a look here Printing latex elements : This isn't so much a project as a useful script. Often in cleaning up latex docs I trawl through the document searching for occurances of say includegraphics . This script will simply print the elements (for example in \\label{eqn: an equation} the element would be eqn: an equation ) to the command line. It takes multiple files and if unspecified will search for proper tex files to use. It has default flags of -f to find figures and -l to find labels, but you can specify whatever you want with -o . For example $ print_tex_elements somearticle.tex -o cite might for example produce Einstein1916 Newton1675","title":"Projects"},{"location":"projects/#projects","text":"In this page, I collect various miscellaneous coding projects I have been involved in. Python for Science A set of workbooks to introduce python for scientific programming koala_html A package to quickly create html pages from collections of images kookaburra A python package for profile-domain timing of radio pulsars parallel_bilby A python package to leverage slurm-based HPC clusters. Enables scaling inference jobs up to many hundreds of cores. bilby_pipe A python package for automating the job of running multiple jobs on LIGO Data Grid clusters. bilby A python package providing a user friendly interface to perform parameter estimation. It is primarily designed and built for inference of compact binary coalescence events in interferometric data, but it can also be used for more general problems. PyFstat A python package containing various methods to run continuous gravitational wave searches. Includes glitch-robust, MCMC-based, and transient work. Bayes Bimodal Test A simple python module using the emcee MCMC software to perform a Bayesian model comparison of bimodality. GitCheck : A python appindicator which provides a visual check of the status of git repos. This builds on some of the functionality of batchgit by Max Hebditch. GetTrainTimes : a command-line tool to quickly get train times from the national rail (UK) website. pyweather : a command-line tool to quickly get a visual (ASCII) forecast of the weather for (almost) any location. Using the Google maps API to study average driving speeds around the globe . To see the results have a look here Printing latex elements : This isn't so much a project as a useful script. Often in cleaning up latex docs I trawl through the document searching for occurances of say includegraphics . This script will simply print the elements (for example in \\label{eqn: an equation} the element would be eqn: an equation ) to the command line. It takes multiple files and if unspecified will search for proper tex files to use. It has default flags of -f to find figures and -l to find labels, but you can specify whatever you want with -o . For example $ print_tex_elements somearticle.tex -o cite might for example produce Einstein1916 Newton1675","title":"Projects"},{"location":"science/","text":"Science Publications You can find all publications to which I have made significant contributions in this ADS library . Workshops May 2023 GWOSC ODW #6 Apr/2021 Royal Hollaway's Astrophysics Residential Aug/2020 Parameter Estimation for Gravitational waves . Lead organizer, invited by the LIGOIndia community to train 60+ astrophysicists May/2020 LIGO-Virgo Collaboration GW Open Data Workshop #3 . Co-organizer, invited to write and coordinate the Parameter Estimation tutorials for 100 students (virtual). Nov/2018 OzGrav workshop: Towards O3. Lead organizer, 20 participants from the OzGrav inference program. A software development sprint. July/2018 OzGrav workshop: Introduction to Inference . Lead organiser, 33 international participants. Training in Bayesian inference and software development. Identifying new projects across the OzGrav nodes and themes.. Miscellaneous Nature Astronomy Community, Behind the Paper: Understanding the rotational evolution of the Vela pulsar during the 2016 glitch Institute of Physics, Gravitational Physics Group 2015 newsletter M. Franchin et al. Current driven nucleation of domain walls in cylindrical nanowires (2011) Selected Presentations Invited to SEPNet workshop Equality, Diversity & Inclusion \u2013 Revisiting the leaky pipeline \u2013 short-term contracts and career planning . My slides are available here . Banff workshop Detection and Analysis of Gravitational Waves in the era of Multi-Messenger Astronomy: From Mathematical Modelling to Machine Learning . You can find a recording here and my slides here . Journal club at CAMK in which I presented Flickering of the Vela pulsar . Seminar at Bar-Ilan University, Israel, January 2021: The deepening mystery of the Vela radio-pulsar glitch GR22/Amaldi13 meeting, Valencia, Spain, July 2019: Gravitational Wave Detection: A Fully Bayesian Approach (contributed) IPTA annual meeting, Pune, India, June 2019: Internal neutron-star physics from the 2016 Vela glitch (contributed, remote) Astrophysics Colloquium, University of Melbourne, October 2018: Astrophysical inference and transient gravitational wave astronomy (invited) ASA Annual Scientific Meeting, Melbourne, Australia, June 2018: Multimessenger follow-up of continuous gravitational wave candidates (contributed) Australasian pulsar meeting, September 2018: Periodic modulations and a glitch in PSR B1828-11 (remote) Institute for Nuclear Theory Workshop INT-18-71W, Astro-Solids, Dense Matter, and Gravitational Waves (April 16 - 20, 2018): Continuous wave parameter estimation and non-standard signal follow up (invited) 11th Bonn workshop on Formation and Evolution of Neutron Stars, Bonn, Germany, December 2017: Neutron stars as continuous gravitational wave emitters Annual NewCompStar Conference , Istanbul, Turkey, 2016: Learning about neutron stars from pulsar precession observations (contributed, best student talk prize) Annual NewCompStar Conference , Budapest, Hungary 2015: Comparing different models of pulsar timing noise (contributed) BritGrav , Birmingham, UK, 2015: Applying Bayesian data analysis to learn about periodic variability in pulsars (contributed) BritGrav , Cambridge, UK, 2014: Gravitational wave searches from noisy neutron stars (contributed, runner up prize for best talk by IoP) Press Interview on Adelaide FiveAA to discuss GW190425, the second binary neutron star event observed by LIGO & Virgo Press for Nature Astronomy article \"Rotational evolution of the Vela pulsar during the 2016 glitch\": The Age: Patient astronomers crack the code of super-dense spinning stars CNET: Astronomers watched a neutron star 'glitch' and can't yet explain it The Register: Mysterious 'glitch' in neutron stars may be down to an itch under the body's surface ABC \"Your Afternoon\" Helen Shield interviews my excellent co-author Jim Palfreyman Phys.org Glitch in neutron star reveals its hidden secrets Astronomy: Astronomers catch a pulsar 'glitching,' offering insights into the strange stars Forbes: A Radio Glitch Reveals The Structure Of A Neutron Star Advocator: Neutron Star Anomaly Revealed More Details On These Mysterious Space Objects ZME Science: Peculiar pulsar slows down before \u2018glitching\u2019 Futarism: A NEUTRON STAR \u201cGLITCHED\u201d \u2014 AND SCIENTISTS NOTICED SOMETHING AMAZING Sci-News: Glitch in Vela Pulsar Provides Unique Opportunity to Study Neutron Star\u2019s Interior Science Daily: Glitch in neutron star reveals its hidden secrets Space.com: Weird Star Slows Down Before 'Glitching,' and No One Knows Why Science Alert: Astronomers Just Got Closer to Unravelling The Mystery of 'Glitching' Pulsars Live Science: Maybe Neutron Stars 'Glitch Out' So Much Because They're Full of Soup IFLS: A Glitch In A Neutron Star Allowed Astronomers To \"Peek\" At Its Interior Spektrum: Wenn Neutronensterne aus dem Takt geraten Physics World: Pulsar glitch suggests superfluid layers lie within neutron star SciShow News: August 16 update","title":"Science"},{"location":"science/#science","text":"","title":"Science"},{"location":"science/#publications","text":"You can find all publications to which I have made significant contributions in this ADS library .","title":"Publications"},{"location":"science/#workshops","text":"May 2023 GWOSC ODW #6 Apr/2021 Royal Hollaway's Astrophysics Residential Aug/2020 Parameter Estimation for Gravitational waves . Lead organizer, invited by the LIGOIndia community to train 60+ astrophysicists May/2020 LIGO-Virgo Collaboration GW Open Data Workshop #3 . Co-organizer, invited to write and coordinate the Parameter Estimation tutorials for 100 students (virtual). Nov/2018 OzGrav workshop: Towards O3. Lead organizer, 20 participants from the OzGrav inference program. A software development sprint. July/2018 OzGrav workshop: Introduction to Inference . Lead organiser, 33 international participants. Training in Bayesian inference and software development. Identifying new projects across the OzGrav nodes and themes..","title":"Workshops"},{"location":"science/#miscellaneous","text":"Nature Astronomy Community, Behind the Paper: Understanding the rotational evolution of the Vela pulsar during the 2016 glitch Institute of Physics, Gravitational Physics Group 2015 newsletter M. Franchin et al. Current driven nucleation of domain walls in cylindrical nanowires (2011)","title":"Miscellaneous"},{"location":"science/#selected-presentations","text":"Invited to SEPNet workshop Equality, Diversity & Inclusion \u2013 Revisiting the leaky pipeline \u2013 short-term contracts and career planning . My slides are available here . Banff workshop Detection and Analysis of Gravitational Waves in the era of Multi-Messenger Astronomy: From Mathematical Modelling to Machine Learning . You can find a recording here and my slides here . Journal club at CAMK in which I presented Flickering of the Vela pulsar . Seminar at Bar-Ilan University, Israel, January 2021: The deepening mystery of the Vela radio-pulsar glitch GR22/Amaldi13 meeting, Valencia, Spain, July 2019: Gravitational Wave Detection: A Fully Bayesian Approach (contributed) IPTA annual meeting, Pune, India, June 2019: Internal neutron-star physics from the 2016 Vela glitch (contributed, remote) Astrophysics Colloquium, University of Melbourne, October 2018: Astrophysical inference and transient gravitational wave astronomy (invited) ASA Annual Scientific Meeting, Melbourne, Australia, June 2018: Multimessenger follow-up of continuous gravitational wave candidates (contributed) Australasian pulsar meeting, September 2018: Periodic modulations and a glitch in PSR B1828-11 (remote) Institute for Nuclear Theory Workshop INT-18-71W, Astro-Solids, Dense Matter, and Gravitational Waves (April 16 - 20, 2018): Continuous wave parameter estimation and non-standard signal follow up (invited) 11th Bonn workshop on Formation and Evolution of Neutron Stars, Bonn, Germany, December 2017: Neutron stars as continuous gravitational wave emitters Annual NewCompStar Conference , Istanbul, Turkey, 2016: Learning about neutron stars from pulsar precession observations (contributed, best student talk prize) Annual NewCompStar Conference , Budapest, Hungary 2015: Comparing different models of pulsar timing noise (contributed) BritGrav , Birmingham, UK, 2015: Applying Bayesian data analysis to learn about periodic variability in pulsars (contributed) BritGrav , Cambridge, UK, 2014: Gravitational wave searches from noisy neutron stars (contributed, runner up prize for best talk by IoP)","title":"Selected Presentations"},{"location":"science/#press","text":"Interview on Adelaide FiveAA to discuss GW190425, the second binary neutron star event observed by LIGO & Virgo Press for Nature Astronomy article \"Rotational evolution of the Vela pulsar during the 2016 glitch\": The Age: Patient astronomers crack the code of super-dense spinning stars CNET: Astronomers watched a neutron star 'glitch' and can't yet explain it The Register: Mysterious 'glitch' in neutron stars may be down to an itch under the body's surface ABC \"Your Afternoon\" Helen Shield interviews my excellent co-author Jim Palfreyman Phys.org Glitch in neutron star reveals its hidden secrets Astronomy: Astronomers catch a pulsar 'glitching,' offering insights into the strange stars Forbes: A Radio Glitch Reveals The Structure Of A Neutron Star Advocator: Neutron Star Anomaly Revealed More Details On These Mysterious Space Objects ZME Science: Peculiar pulsar slows down before \u2018glitching\u2019 Futarism: A NEUTRON STAR \u201cGLITCHED\u201d \u2014 AND SCIENTISTS NOTICED SOMETHING AMAZING Sci-News: Glitch in Vela Pulsar Provides Unique Opportunity to Study Neutron Star\u2019s Interior Science Daily: Glitch in neutron star reveals its hidden secrets Space.com: Weird Star Slows Down Before 'Glitching,' and No One Knows Why Science Alert: Astronomers Just Got Closer to Unravelling The Mystery of 'Glitching' Pulsars Live Science: Maybe Neutron Stars 'Glitch Out' So Much Because They're Full of Soup IFLS: A Glitch In A Neutron Star Allowed Astronomers To \"Peek\" At Its Interior Spektrum: Wenn Neutronensterne aus dem Takt geraten Physics World: Pulsar glitch suggests superfluid layers lie within neutron star SciShow News: August 16 update","title":"Press"},{"location":"notes/splitting_up_mrs/","text":"Splitting up a large Merge Request It is common when developing a new feature on a branch to end up fixing a number of bugs or extending functionality elsewhere in the code. After you are done, however, the code reviewers may well ask that the bug fixes be separated into a separate merge request. This makes their life easier. They can review the bug fixes and get these merges into master quickly. Meanwhile, your new feature may need a more in-depth review. It would not be sensible to hold up fixing bugs based on the review of new code. In this note, I'll describe the method I use to break such large merge requests up. Advice on merge requests in general can be found here . Figure out where the changes are Let's say you are developing on feature-branch and want to merge into main-branch . First, let's figure out which files have been modified: $ git diff --name-only main-branch feature-branch src/utils.py src/magic.py Here we can see that both utils.py and magic.py have been modified. Let's say that the changes to utils.py where all bug fixes while magic.py is a new file introducing new features. The code reviewers want to split up our merge request into two: one which only adds the bug fixes to utils.py and a second one which adds the new functionality in magic.py . The Problem and Solution The problem, from our perspective, is that the changes to magic.py depend on the changes to utils.py ! Of course, we fixed the bugs so that it would run. To satisfy the code reviewers while not breaking our branch, we need to: Create a new merge request with only the changes to utils.py included Request the new merge request be merged into master Rebase our feature-branch to master (bringing with it the changes to utils.py ) Now we have a feature-branch with only magic.py being changed, but all the while feature-branch never got broken. Technical how-to The steps above can be done as follows: Create a new merge request with only the changes to utils.py included $ (main-branch) git checkout main-branch $ (main-branch) git checkout -b fix-utils # Create branch of main-branch $ (fix-utils) git diff main-branch feature-branch -- src/utils.py > utils_changes.patch # Write the diff between the main and feature branches to a patch $ (fix-utils) git apply utils_changes.patch # Apply the patch $ (fix-utils) git commit -m \"Fixing utils\" Now you can push this branch and create a merge request. Once fix-utils gets merged into main-branch , all you need to do is rebase your feature-branch to main-branch and then your merge request will only contain changes to magic.py . Magic.","title":"Splitting up a large Merge Request"},{"location":"notes/splitting_up_mrs/#splitting-up-a-large-merge-request","text":"It is common when developing a new feature on a branch to end up fixing a number of bugs or extending functionality elsewhere in the code. After you are done, however, the code reviewers may well ask that the bug fixes be separated into a separate merge request. This makes their life easier. They can review the bug fixes and get these merges into master quickly. Meanwhile, your new feature may need a more in-depth review. It would not be sensible to hold up fixing bugs based on the review of new code. In this note, I'll describe the method I use to break such large merge requests up. Advice on merge requests in general can be found here .","title":"Splitting up a large Merge Request"},{"location":"notes/splitting_up_mrs/#figure-out-where-the-changes-are","text":"Let's say you are developing on feature-branch and want to merge into main-branch . First, let's figure out which files have been modified: $ git diff --name-only main-branch feature-branch src/utils.py src/magic.py Here we can see that both utils.py and magic.py have been modified. Let's say that the changes to utils.py where all bug fixes while magic.py is a new file introducing new features. The code reviewers want to split up our merge request into two: one which only adds the bug fixes to utils.py and a second one which adds the new functionality in magic.py .","title":"Figure out where the changes are"},{"location":"notes/splitting_up_mrs/#the-problem-and-solution","text":"The problem, from our perspective, is that the changes to magic.py depend on the changes to utils.py ! Of course, we fixed the bugs so that it would run. To satisfy the code reviewers while not breaking our branch, we need to: Create a new merge request with only the changes to utils.py included Request the new merge request be merged into master Rebase our feature-branch to master (bringing with it the changes to utils.py ) Now we have a feature-branch with only magic.py being changed, but all the while feature-branch never got broken.","title":"The Problem and Solution"},{"location":"notes/splitting_up_mrs/#technical-how-to","text":"The steps above can be done as follows: Create a new merge request with only the changes to utils.py included $ (main-branch) git checkout main-branch $ (main-branch) git checkout -b fix-utils # Create branch of main-branch $ (fix-utils) git diff main-branch feature-branch -- src/utils.py > utils_changes.patch # Write the diff between the main and feature branches to a patch $ (fix-utils) git apply utils_changes.patch # Apply the patch $ (fix-utils) git commit -m \"Fixing utils\" Now you can push this branch and create a merge request. Once fix-utils gets merged into main-branch , all you need to do is rebase your feature-branch to main-branch and then your merge request will only contain changes to magic.py . Magic.","title":"Technical how-to"},{"location":"notes/waveform_projection/","text":"! pip install bilby lalsuite import lalsimulation as lalsim import lal import bilby import matplotlib.pyplot as plt import numpy as np from scipy.interpolate import interp1d solar_mass = bilby . core . utils . constants . solar_mass parsec = bilby . core . utils . constants . parsec Generating time domain waveforms and projecting them onto the detectors In this notebook, I'll demonstrate how to use lalsimulation to generate a time-domain waveform and how to then use bilby to calculate the response of an interferometer. Setting a few things up To get started, let's define some properties of the data we want to simulate. This is the sampling frequency, duration and the amount of time before and after the trigger time (roughly speaking, the peak of the 2,2 mode). sampling_frequency = 4096 deltaT = 1 / sampling_frequency duration = 4 post_trigger_duration = 0.5 pre_trigger_duration = duration - post_trigger_duration Generate a waveform Now, we'll use lalsim.SimInspiralChooseTDWaveform to generate the plus and cross polarization # Define the parameters in SI units mass_1 = 30 * solar_mass mass_2 = 30 * solar_mass spin_1x , spin_1y , spin_1z = 0 , 0 , 0 spin_2x , spin_2y , spin_2z = 0 , 0 , 0 luminosity_distance = 2000 theta_jn = 0 phase = 0 longAscNodes = 0 eccentricity = 0 meanPerAno = 0 LALParams = lal . CreateDict () # Get the approximant number from the name waveform_approximant = \"IMRPhenomT\" approximant = lalsim . GetApproximantFromString ( waveform_approximant ) # Estimate a minimum frequency required to ensure the waveform covers the data # Note the 0.95 is a fudge factor as SimInspiralChirpStartFrequencyBound includes # only the leading order Newtonian coefficient. f_min = 0.95 * lalsim . SimInspiralChirpStartFrequencyBound ( pre_trigger_duration , mass_1 , mass_2 ) if lalsim . SimInspiralGetSpinFreqFromApproximant ( approximant ) == lalsim . SIM_INSPIRAL_SPINS_FLOW : f_ref = f_min else : f_ref = 20 h_plus_timeseries , h_cross_timeseries = lalsim . SimInspiralChooseTDWaveform ( mass_1 , mass_2 , spin_1x , spin_1y , spin_1z , spin_2x , spin_2y , spin_2z , luminosity_distance , theta_jn , phase , longAscNodes , eccentricity , meanPerAno , deltaT , f_min , f_ref , LALParams , approximant ) Extract the data h_plus = h_plus_timeseries . data . data h_cross = h_cross_timeseries . data . data h_plus_time = np . arange ( len ( h_plus )) * h_plus_timeseries . deltaT + float ( h_plus_timeseries . epoch ) h_cross_time = np . arange ( len ( h_cross )) * h_cross_timeseries . deltaT + float ( h_cross_timeseries . epoch ) Project onto the Hanford interferometer ra = 1.2 dec = - 3.1 geocent_time = 1126259462.1 psi = 0.5 H1 = bilby . gw . detector . get_empty_interferometer ( \"H1\" ) plus_polarization_tensor = bilby . gw . utils . get_polarization_tensor ( ra , dec , geocent_time , psi , \"plus\" ) f_plus = np . einsum ( 'ij,ij->' , H1 . detector_tensor , plus_polarization_tensor ) cross_polarization_tensor = bilby . gw . utils . get_polarization_tensor ( ra , dec , geocent_time , psi , \"cross\" ) f_cross = np . einsum ( 'ij,ij->' , H1 . detector_tensor , cross_polarization_tensor ) strain = f_plus * h_plus + f_cross * h_cross strain_time = h_plus_time Plot the data plt . plot ( strain_time , strain ) plt . ylabel ( \"Strain\" ) plt . xlabel ( f \"GPS time [s]\" ) plt . show () From the plot above (or by inspecting strain and strain_time ), we see that SimInspiralChooseTDWaveform outputs the strain on a grid of times with sampling frequency 1/deltaT , but that the duration is determined by f_min and that peak of the 2,2 mode occurs at 0 . We can translate this to the time measured by a detector by simply adding geocent_time , e.g. strain_detector_time = strain_time + geocent_time But, we'll want to compare our predicted strain with a timeseries of detector data which will be sampled on a different grid (even if the sampling frequency is identical, we would not expect a sampled timeseries to align with the peak of the 2,2 mode!). To convert, we can interpolate. Interpolate onto a sampled data grid n = sampling_frequency * duration data_start_time = int ( geocent_time ) - pre_trigger_duration data_detector_time = np . arange ( n ) / sampling_frequency + data_start_time h_interp = interp1d ( strain_detector_time , strain , fill_value = 0 , bounds_error = False )( data_detector_time ) fig , ( ax1 , ax2 ) = plt . subplots ( ncols = 2 , figsize = ( 15 , 6 )) ax1 . plot ( data_detector_time - geocent_time , h_interp ) ax1 . plot ( strain_detector_time - geocent_time , strain , \"--\" ) ax1 . set_ylabel ( \"Strain\" ) ax1 . set_xlabel ( f \"GPS time - { geocent_time } [s]\" ) ax2 . plot ( data_detector_time - geocent_time , h_interp , label = \"Interpolated\" ) ax2 . plot ( strain_detector_time - geocent_time , strain , \"--\" , label = \"Strain\" ) ax2 . set_ylabel ( \"Strain\" ) ax2 . set_xlabel ( f \"GPS time - { geocent_time } [s]\" ) ax2 . set_xlim ( - 0.1 , 0.1 ) ax2 . legend () plt . show () Putting it all together into a single function from bilby.gw.utils import _get_lalsim_approximant , convert_args_list_to_float def get_gw_waveform ( time , parameters , waveform_approximant , reference_frequency , bilby_detector , fudge = 0.95 , reference_frame = None , pre_trigger_duration = None , error = False ): par , _ = bilby . gw . conversion . convert_to_lal_binary_black_hole_parameters ( parameters ) mass_1_SI = par [ \"mass_1\" ] * solar_mass mass_2_SI = par [ \"mass_2\" ] * solar_mass luminosity_distance_SI = par [ \"luminosity_distance\" ] * 1e6 * parsec # Extract information about the time series deltaT = time [ 1 ] - time [ 0 ] if pre_trigger_duration is None : nearest_trigger_idx = np . argmin ( np . abs ( time - par [ \"geocent_time\" ])) pre_trigger_duration = time [ nearest_trigger_idx ] - time [ 0 ] # Get the approximant number from the name approximant = _get_lalsim_approximant ( waveform_approximant ) # Estimate a minimum frequency required to ensure the waveform covers the data # Note the 0.95 is a fudge factor as SimInspiralChirpStartFrequencyBound includes # only the leading order Newtonian coefficient. f_min = fudge * lalsim . SimInspiralChirpStartFrequencyBound ( pre_trigger_duration , mass_1_SI , mass_2_SI , ) # Check if the reference frequency is used, if not use f_min if lalsim . SimInspiralGetSpinFreqFromApproximant ( approximant ) == lalsim . SIM_INSPIRAL_SPINS_FLOW : f_ref = f_min elif reference_frequency == \"fmin\" : f_ref = f_min else : f_ref = reference_frequency iota , spin_1x , spin_1y , spin_1z , spin_2x , spin_2y , spin_2z = bilby . gw . conversion . bilby_to_lalsimulation_spins ( theta_jn = par [ \"theta_jn\" ], phi_jl = par [ \"phi_jl\" ], tilt_1 = par [ \"tilt_1\" ], tilt_2 = par [ \"tilt_2\" ], phi_12 = par [ \"phi_12\" ], a_1 = par [ \"a_1\" ], a_2 = par [ \"a_2\" ], mass_1 = mass_1_SI , mass_2 = mass_2_SI , reference_frequency = f_ref , phase = par [ \"phase\" ]) if \"zenith\" in par and \"azimuth\" in par : par [ \"ra\" ], par [ \"dec\" ] = bilby . gw . utils . zenith_azimuth_to_ra_dec ( par [ 'zenith' ], par [ 'azimuth' ], par [ \"geocent_time\" ], reference_frame ) longitude_ascending_nodes = 0. eccentricity = 0. mean_per_ano = 0. waveform_dictionary = lal . CreateDict () args = convert_args_list_to_float ( mass_1_SI , mass_2_SI , spin_1x , spin_1y , spin_1z , spin_2x , spin_2y , spin_2z , luminosity_distance_SI , iota , par [ \"phase\" ], longitude_ascending_nodes , eccentricity , mean_per_ano , deltaT , f_min , f_ref ) h_plus_timeseries , h_cross_timeseries = lalsim . SimInspiralChooseTDWaveform ( * args , waveform_dictionary , approximant ) plus_polarization_tensor = bilby . gw . utils . get_polarization_tensor ( par [ \"ra\" ], par [ \"dec\" ], par [ \"geocent_time\" ], par [ \"psi\" ], \"plus\" ) f_plus = np . einsum ( 'ij,ij->' , bilby_detector . detector_tensor , plus_polarization_tensor ) cross_polarization_tensor = bilby . gw . utils . get_polarization_tensor ( par [ \"ra\" ], par [ \"dec\" ], par [ \"geocent_time\" ], par [ \"psi\" ], \"cross\" ) f_cross = np . einsum ( 'ij,ij->' , bilby_detector . detector_tensor , cross_polarization_tensor ) h_plus = h_plus_timeseries . data . data h_cross = h_cross_timeseries . data . data h_plus_time = np . arange ( len ( h_plus )) * h_plus_timeseries . deltaT + float ( h_plus_timeseries . epoch ) h = f_plus * h_plus + f_cross * h_cross t = h_plus_time + par [ \"geocent_time\" ] h_interp = interp1d ( t , h , fill_value = 0 , bounds_error = False )( time ) if h_interp [ 0 ] == 0 : msg = \"Generated waveform was too short\" if error : raise ValueError ( msg ) else : print ( msg ) return h_interp parameters = dict ( mass_1 = 36. , mass_2 = 29. , chi_1 = 0.4 , chi_2 = 0.3 , luminosity_distance = 2000. , theta_jn = 0.4 , psi = 2.659 , phase = 2.8 , geocent_time = geocent_time , ra = 1.375 , dec =- 1.2108 ) for waveform in [ \"IMRPhenomT\" , \"SEOBNRv4T\" , \"TEOBResumS\" ]: w = get_gw_waveform ( data_detector_time , parameters , waveform , \"fmin\" , H1 ) plt . plot ( data_detector_time - geocent_time , w , label = waveform ) plt . xlim ( - 0.1 , 0.05 ) plt . ylabel ( \"Strain\" ) plt . xlabel ( f \"GPS time - { geocent_time } [s]\" ) plt . legend () plt . show () Time the generation prior = bilby . gw . prior . BBHPriorDict () prior [ \"geocent_time\" ] = bilby . core . prior . Uniform ( geocent_time - 0.1 , geocent_time + 0.1 ) 10:29 bilby INFO : No prior given, using default BBH priors in /home/greg/bilby/bilby/gw/prior_files/precessing_spins_bbh.prior. %% timeit _ = get_gw_waveform ( data_detector_time , prior . sample (), \"SEOBNRv4P\" , 20 , H1 , fudge = 0.8 ) 1.28 s \u00b1 96.5 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) %% timeit _ = get_gw_waveform ( data_detector_time , prior . sample (), \"IMRPhenomTP\" , 20 , H1 , fudge = 0.8 ) 30.9 ms \u00b1 2.82 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) %% timeit _ = get_gw_waveform ( data_detector_time , prior . sample (), \"IMRPhenomPv2\" , 20 , H1 , fudge = 0.8 , pre_trigger_duration = pre_trigger_duration ) 13.6 ms \u00b1 237 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) Profile the generation % load_ext line_profiler % lprun - T profile . txt - s - u 1e-3 - f get_gw_waveform get_gw_waveform ( data_detector_time , prior . sample (), \"IMRPhenomTP\" , 20 , H1 , fudge = 0.85 ) *** Profile printout saved to text file 'profile.txt'. ! head - n 15 profile . txt Timer unit: 0.001 s Total time: 0.029128 s File: /tmp/ipykernel_1170/3049955836.py Function: get_gw_waveform at line 3 Line # Hits Time Per Hit % Time Line Contents ============================================================== 3 def get_gw_waveform(time, parameters, waveform_approximant, reference_frequency, bilby_detector, fudge=0.95, reference_frame=None, pre_trigger_duration=None, error=False): 4 1 0.1 0.1 0.2 par, _ = bilby.gw.conversion.convert_to_lal_binary_black_hole_parameters(parameters) 5 6 1 0.0 0.0 0.0 mass_1_SI = par[\"mass_1\"] * solar_mass 7 1 0.0 0.0 0.0 mass_2_SI = par[\"mass_2\"] * solar_mass 8 1 0.0 0.0 0.0 luminosity_distance_SI = par[\"luminosity_distance\"] * 1e6 * parsec 9 ! tail - n + 9 profile . txt | sort - nr - k 3 | head - n 10 60 4 27.6 6.9 94.7 h_plus_timeseries, h_cross_timeseries = lalsim.SimInspiralChooseTDWaveform( 77 1 0.6 0.6 2.0 h_interp = interp1d(t, h, fill_value=0, bounds_error=False)(time) 64 1 0.3 0.3 0.9 plus_polarization_tensor = bilby.gw.utils.get_polarization_tensor(par[\"ra\"], par[\"dec\"], par[\"geocent_time\"], par[\"psi\"], \"plus\") 38 2 0.2 0.1 0.7 iota, spin_1x, spin_1y, spin_1z, spin_2x, spin_2y, spin_2z = bilby.gw.conversion.bilby_to_lalsimulation_spins( 74 1 0.1 0.1 0.3 h = f_plus * h_plus + f_cross * h_cross 72 1 0.1 0.1 0.3 h_plus_time = np.arange(len(h_plus)) * h_plus_timeseries.deltaT + float(h_plus_timeseries.epoch) 67 1 0.1 0.1 0.2 cross_polarization_tensor = bilby.gw.utils.get_polarization_tensor(par[\"ra\"], par[\"dec\"], par[\"geocent_time\"], par[\"psi\"], \"cross\") 13 1 0.1 0.1 0.4 nearest_trigger_idx = np.argmin(np.abs(time - par[\"geocent_time\"])) 4 1 0.1 0.1 0.2 par, _ = bilby.gw.conversion.convert_to_lal_binary_black_hole_parameters(parameters) 85 1 0.0 0.0 0.0 return h_interp We can remove that lookup for the pre_trigger_duration if we know it % lprun - T profile . txt - s - u 1e-3 - f get_gw_waveform get_gw_waveform ( data_detector_time , prior . sample (), \"IMRPhenomTP\" , 20 , H1 , fudge = 0.85 , pre_trigger_duration = pre_trigger_duration ) ! tail - n + 9 profile . txt | sort - nr - k 3 | head - n 10 *** Profile printout saved to text file 'profile.txt'. 60 4 38.3 9.6 91.5 h_plus_timeseries, h_cross_timeseries = lalsim.SimInspiralChooseTDWaveform( 77 1 1.5 1.5 3.7 h_interp = interp1d(t, h, fill_value=0, bounds_error=False)(time) 74 1 0.8 0.8 2.0 h = f_plus * h_plus + f_cross * h_cross 38 2 0.4 0.2 0.9 iota, spin_1x, spin_1y, spin_1z, spin_2x, spin_2y, spin_2z = bilby.gw.conversion.bilby_to_lalsimulation_spins( 64 1 0.2 0.2 0.6 plus_polarization_tensor = bilby.gw.utils.get_polarization_tensor(par[\"ra\"], par[\"dec\"], par[\"geocent_time\"], par[\"psi\"], \"plus\") 4 1 0.2 0.2 0.4 par, _ = bilby.gw.conversion.convert_to_lal_binary_black_hole_parameters(parameters) 75 1 0.1 0.1 0.1 t = h_plus_time + par[\"geocent_time\"] 72 1 0.1 0.1 0.3 h_plus_time = np.arange(len(h_plus)) * h_plus_timeseries.deltaT + float(h_plus_timeseries.epoch) 85 1 0.0 0.0 0.0 return h_interp 84 print(msg)","title":"Waveform projection"},{"location":"notes/waveform_projection/#generating-time-domain-waveforms-and-projecting-them-onto-the-detectors","text":"In this notebook, I'll demonstrate how to use lalsimulation to generate a time-domain waveform and how to then use bilby to calculate the response of an interferometer.","title":"Generating time domain waveforms and projecting them onto the detectors"},{"location":"notes/waveform_projection/#setting-a-few-things-up","text":"To get started, let's define some properties of the data we want to simulate. This is the sampling frequency, duration and the amount of time before and after the trigger time (roughly speaking, the peak of the 2,2 mode). sampling_frequency = 4096 deltaT = 1 / sampling_frequency duration = 4 post_trigger_duration = 0.5 pre_trigger_duration = duration - post_trigger_duration","title":"Setting a few things up"},{"location":"notes/waveform_projection/#generate-a-waveform","text":"Now, we'll use lalsim.SimInspiralChooseTDWaveform to generate the plus and cross polarization # Define the parameters in SI units mass_1 = 30 * solar_mass mass_2 = 30 * solar_mass spin_1x , spin_1y , spin_1z = 0 , 0 , 0 spin_2x , spin_2y , spin_2z = 0 , 0 , 0 luminosity_distance = 2000 theta_jn = 0 phase = 0 longAscNodes = 0 eccentricity = 0 meanPerAno = 0 LALParams = lal . CreateDict () # Get the approximant number from the name waveform_approximant = \"IMRPhenomT\" approximant = lalsim . GetApproximantFromString ( waveform_approximant ) # Estimate a minimum frequency required to ensure the waveform covers the data # Note the 0.95 is a fudge factor as SimInspiralChirpStartFrequencyBound includes # only the leading order Newtonian coefficient. f_min = 0.95 * lalsim . SimInspiralChirpStartFrequencyBound ( pre_trigger_duration , mass_1 , mass_2 ) if lalsim . SimInspiralGetSpinFreqFromApproximant ( approximant ) == lalsim . SIM_INSPIRAL_SPINS_FLOW : f_ref = f_min else : f_ref = 20 h_plus_timeseries , h_cross_timeseries = lalsim . SimInspiralChooseTDWaveform ( mass_1 , mass_2 , spin_1x , spin_1y , spin_1z , spin_2x , spin_2y , spin_2z , luminosity_distance , theta_jn , phase , longAscNodes , eccentricity , meanPerAno , deltaT , f_min , f_ref , LALParams , approximant )","title":"Generate a waveform"},{"location":"notes/waveform_projection/#extract-the-data","text":"h_plus = h_plus_timeseries . data . data h_cross = h_cross_timeseries . data . data h_plus_time = np . arange ( len ( h_plus )) * h_plus_timeseries . deltaT + float ( h_plus_timeseries . epoch ) h_cross_time = np . arange ( len ( h_cross )) * h_cross_timeseries . deltaT + float ( h_cross_timeseries . epoch )","title":"Extract the data"},{"location":"notes/waveform_projection/#project-onto-the-hanford-interferometer","text":"ra = 1.2 dec = - 3.1 geocent_time = 1126259462.1 psi = 0.5 H1 = bilby . gw . detector . get_empty_interferometer ( \"H1\" ) plus_polarization_tensor = bilby . gw . utils . get_polarization_tensor ( ra , dec , geocent_time , psi , \"plus\" ) f_plus = np . einsum ( 'ij,ij->' , H1 . detector_tensor , plus_polarization_tensor ) cross_polarization_tensor = bilby . gw . utils . get_polarization_tensor ( ra , dec , geocent_time , psi , \"cross\" ) f_cross = np . einsum ( 'ij,ij->' , H1 . detector_tensor , cross_polarization_tensor ) strain = f_plus * h_plus + f_cross * h_cross strain_time = h_plus_time","title":"Project onto the Hanford interferometer"},{"location":"notes/waveform_projection/#plot-the-data","text":"plt . plot ( strain_time , strain ) plt . ylabel ( \"Strain\" ) plt . xlabel ( f \"GPS time [s]\" ) plt . show () From the plot above (or by inspecting strain and strain_time ), we see that SimInspiralChooseTDWaveform outputs the strain on a grid of times with sampling frequency 1/deltaT , but that the duration is determined by f_min and that peak of the 2,2 mode occurs at 0 . We can translate this to the time measured by a detector by simply adding geocent_time , e.g. strain_detector_time = strain_time + geocent_time But, we'll want to compare our predicted strain with a timeseries of detector data which will be sampled on a different grid (even if the sampling frequency is identical, we would not expect a sampled timeseries to align with the peak of the 2,2 mode!). To convert, we can interpolate.","title":"Plot the data"},{"location":"notes/waveform_projection/#interpolate-onto-a-sampled-data-grid","text":"n = sampling_frequency * duration data_start_time = int ( geocent_time ) - pre_trigger_duration data_detector_time = np . arange ( n ) / sampling_frequency + data_start_time h_interp = interp1d ( strain_detector_time , strain , fill_value = 0 , bounds_error = False )( data_detector_time ) fig , ( ax1 , ax2 ) = plt . subplots ( ncols = 2 , figsize = ( 15 , 6 )) ax1 . plot ( data_detector_time - geocent_time , h_interp ) ax1 . plot ( strain_detector_time - geocent_time , strain , \"--\" ) ax1 . set_ylabel ( \"Strain\" ) ax1 . set_xlabel ( f \"GPS time - { geocent_time } [s]\" ) ax2 . plot ( data_detector_time - geocent_time , h_interp , label = \"Interpolated\" ) ax2 . plot ( strain_detector_time - geocent_time , strain , \"--\" , label = \"Strain\" ) ax2 . set_ylabel ( \"Strain\" ) ax2 . set_xlabel ( f \"GPS time - { geocent_time } [s]\" ) ax2 . set_xlim ( - 0.1 , 0.1 ) ax2 . legend () plt . show ()","title":"Interpolate onto a sampled data grid"},{"location":"notes/waveform_projection/#putting-it-all-together-into-a-single-function","text":"from bilby.gw.utils import _get_lalsim_approximant , convert_args_list_to_float def get_gw_waveform ( time , parameters , waveform_approximant , reference_frequency , bilby_detector , fudge = 0.95 , reference_frame = None , pre_trigger_duration = None , error = False ): par , _ = bilby . gw . conversion . convert_to_lal_binary_black_hole_parameters ( parameters ) mass_1_SI = par [ \"mass_1\" ] * solar_mass mass_2_SI = par [ \"mass_2\" ] * solar_mass luminosity_distance_SI = par [ \"luminosity_distance\" ] * 1e6 * parsec # Extract information about the time series deltaT = time [ 1 ] - time [ 0 ] if pre_trigger_duration is None : nearest_trigger_idx = np . argmin ( np . abs ( time - par [ \"geocent_time\" ])) pre_trigger_duration = time [ nearest_trigger_idx ] - time [ 0 ] # Get the approximant number from the name approximant = _get_lalsim_approximant ( waveform_approximant ) # Estimate a minimum frequency required to ensure the waveform covers the data # Note the 0.95 is a fudge factor as SimInspiralChirpStartFrequencyBound includes # only the leading order Newtonian coefficient. f_min = fudge * lalsim . SimInspiralChirpStartFrequencyBound ( pre_trigger_duration , mass_1_SI , mass_2_SI , ) # Check if the reference frequency is used, if not use f_min if lalsim . SimInspiralGetSpinFreqFromApproximant ( approximant ) == lalsim . SIM_INSPIRAL_SPINS_FLOW : f_ref = f_min elif reference_frequency == \"fmin\" : f_ref = f_min else : f_ref = reference_frequency iota , spin_1x , spin_1y , spin_1z , spin_2x , spin_2y , spin_2z = bilby . gw . conversion . bilby_to_lalsimulation_spins ( theta_jn = par [ \"theta_jn\" ], phi_jl = par [ \"phi_jl\" ], tilt_1 = par [ \"tilt_1\" ], tilt_2 = par [ \"tilt_2\" ], phi_12 = par [ \"phi_12\" ], a_1 = par [ \"a_1\" ], a_2 = par [ \"a_2\" ], mass_1 = mass_1_SI , mass_2 = mass_2_SI , reference_frequency = f_ref , phase = par [ \"phase\" ]) if \"zenith\" in par and \"azimuth\" in par : par [ \"ra\" ], par [ \"dec\" ] = bilby . gw . utils . zenith_azimuth_to_ra_dec ( par [ 'zenith' ], par [ 'azimuth' ], par [ \"geocent_time\" ], reference_frame ) longitude_ascending_nodes = 0. eccentricity = 0. mean_per_ano = 0. waveform_dictionary = lal . CreateDict () args = convert_args_list_to_float ( mass_1_SI , mass_2_SI , spin_1x , spin_1y , spin_1z , spin_2x , spin_2y , spin_2z , luminosity_distance_SI , iota , par [ \"phase\" ], longitude_ascending_nodes , eccentricity , mean_per_ano , deltaT , f_min , f_ref ) h_plus_timeseries , h_cross_timeseries = lalsim . SimInspiralChooseTDWaveform ( * args , waveform_dictionary , approximant ) plus_polarization_tensor = bilby . gw . utils . get_polarization_tensor ( par [ \"ra\" ], par [ \"dec\" ], par [ \"geocent_time\" ], par [ \"psi\" ], \"plus\" ) f_plus = np . einsum ( 'ij,ij->' , bilby_detector . detector_tensor , plus_polarization_tensor ) cross_polarization_tensor = bilby . gw . utils . get_polarization_tensor ( par [ \"ra\" ], par [ \"dec\" ], par [ \"geocent_time\" ], par [ \"psi\" ], \"cross\" ) f_cross = np . einsum ( 'ij,ij->' , bilby_detector . detector_tensor , cross_polarization_tensor ) h_plus = h_plus_timeseries . data . data h_cross = h_cross_timeseries . data . data h_plus_time = np . arange ( len ( h_plus )) * h_plus_timeseries . deltaT + float ( h_plus_timeseries . epoch ) h = f_plus * h_plus + f_cross * h_cross t = h_plus_time + par [ \"geocent_time\" ] h_interp = interp1d ( t , h , fill_value = 0 , bounds_error = False )( time ) if h_interp [ 0 ] == 0 : msg = \"Generated waveform was too short\" if error : raise ValueError ( msg ) else : print ( msg ) return h_interp parameters = dict ( mass_1 = 36. , mass_2 = 29. , chi_1 = 0.4 , chi_2 = 0.3 , luminosity_distance = 2000. , theta_jn = 0.4 , psi = 2.659 , phase = 2.8 , geocent_time = geocent_time , ra = 1.375 , dec =- 1.2108 ) for waveform in [ \"IMRPhenomT\" , \"SEOBNRv4T\" , \"TEOBResumS\" ]: w = get_gw_waveform ( data_detector_time , parameters , waveform , \"fmin\" , H1 ) plt . plot ( data_detector_time - geocent_time , w , label = waveform ) plt . xlim ( - 0.1 , 0.05 ) plt . ylabel ( \"Strain\" ) plt . xlabel ( f \"GPS time - { geocent_time } [s]\" ) plt . legend () plt . show ()","title":"Putting it all together into a single function"},{"location":"notes/waveform_projection/#time-the-generation","text":"prior = bilby . gw . prior . BBHPriorDict () prior [ \"geocent_time\" ] = bilby . core . prior . Uniform ( geocent_time - 0.1 , geocent_time + 0.1 ) 10:29 bilby INFO : No prior given, using default BBH priors in /home/greg/bilby/bilby/gw/prior_files/precessing_spins_bbh.prior. %% timeit _ = get_gw_waveform ( data_detector_time , prior . sample (), \"SEOBNRv4P\" , 20 , H1 , fudge = 0.8 ) 1.28 s \u00b1 96.5 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) %% timeit _ = get_gw_waveform ( data_detector_time , prior . sample (), \"IMRPhenomTP\" , 20 , H1 , fudge = 0.8 ) 30.9 ms \u00b1 2.82 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) %% timeit _ = get_gw_waveform ( data_detector_time , prior . sample (), \"IMRPhenomPv2\" , 20 , H1 , fudge = 0.8 , pre_trigger_duration = pre_trigger_duration ) 13.6 ms \u00b1 237 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)","title":"Time the generation"},{"location":"notes/waveform_projection/#profile-the-generation","text":"% load_ext line_profiler % lprun - T profile . txt - s - u 1e-3 - f get_gw_waveform get_gw_waveform ( data_detector_time , prior . sample (), \"IMRPhenomTP\" , 20 , H1 , fudge = 0.85 ) *** Profile printout saved to text file 'profile.txt'. ! head - n 15 profile . txt Timer unit: 0.001 s Total time: 0.029128 s File: /tmp/ipykernel_1170/3049955836.py Function: get_gw_waveform at line 3 Line # Hits Time Per Hit % Time Line Contents ============================================================== 3 def get_gw_waveform(time, parameters, waveform_approximant, reference_frequency, bilby_detector, fudge=0.95, reference_frame=None, pre_trigger_duration=None, error=False): 4 1 0.1 0.1 0.2 par, _ = bilby.gw.conversion.convert_to_lal_binary_black_hole_parameters(parameters) 5 6 1 0.0 0.0 0.0 mass_1_SI = par[\"mass_1\"] * solar_mass 7 1 0.0 0.0 0.0 mass_2_SI = par[\"mass_2\"] * solar_mass 8 1 0.0 0.0 0.0 luminosity_distance_SI = par[\"luminosity_distance\"] * 1e6 * parsec 9 ! tail - n + 9 profile . txt | sort - nr - k 3 | head - n 10 60 4 27.6 6.9 94.7 h_plus_timeseries, h_cross_timeseries = lalsim.SimInspiralChooseTDWaveform( 77 1 0.6 0.6 2.0 h_interp = interp1d(t, h, fill_value=0, bounds_error=False)(time) 64 1 0.3 0.3 0.9 plus_polarization_tensor = bilby.gw.utils.get_polarization_tensor(par[\"ra\"], par[\"dec\"], par[\"geocent_time\"], par[\"psi\"], \"plus\") 38 2 0.2 0.1 0.7 iota, spin_1x, spin_1y, spin_1z, spin_2x, spin_2y, spin_2z = bilby.gw.conversion.bilby_to_lalsimulation_spins( 74 1 0.1 0.1 0.3 h = f_plus * h_plus + f_cross * h_cross 72 1 0.1 0.1 0.3 h_plus_time = np.arange(len(h_plus)) * h_plus_timeseries.deltaT + float(h_plus_timeseries.epoch) 67 1 0.1 0.1 0.2 cross_polarization_tensor = bilby.gw.utils.get_polarization_tensor(par[\"ra\"], par[\"dec\"], par[\"geocent_time\"], par[\"psi\"], \"cross\") 13 1 0.1 0.1 0.4 nearest_trigger_idx = np.argmin(np.abs(time - par[\"geocent_time\"])) 4 1 0.1 0.1 0.2 par, _ = bilby.gw.conversion.convert_to_lal_binary_black_hole_parameters(parameters) 85 1 0.0 0.0 0.0 return h_interp We can remove that lookup for the pre_trigger_duration if we know it % lprun - T profile . txt - s - u 1e-3 - f get_gw_waveform get_gw_waveform ( data_detector_time , prior . sample (), \"IMRPhenomTP\" , 20 , H1 , fudge = 0.85 , pre_trigger_duration = pre_trigger_duration ) ! tail - n + 9 profile . txt | sort - nr - k 3 | head - n 10 *** Profile printout saved to text file 'profile.txt'. 60 4 38.3 9.6 91.5 h_plus_timeseries, h_cross_timeseries = lalsim.SimInspiralChooseTDWaveform( 77 1 1.5 1.5 3.7 h_interp = interp1d(t, h, fill_value=0, bounds_error=False)(time) 74 1 0.8 0.8 2.0 h = f_plus * h_plus + f_cross * h_cross 38 2 0.4 0.2 0.9 iota, spin_1x, spin_1y, spin_1z, spin_2x, spin_2y, spin_2z = bilby.gw.conversion.bilby_to_lalsimulation_spins( 64 1 0.2 0.2 0.6 plus_polarization_tensor = bilby.gw.utils.get_polarization_tensor(par[\"ra\"], par[\"dec\"], par[\"geocent_time\"], par[\"psi\"], \"plus\") 4 1 0.2 0.2 0.4 par, _ = bilby.gw.conversion.convert_to_lal_binary_black_hole_parameters(parameters) 75 1 0.1 0.1 0.1 t = h_plus_time + par[\"geocent_time\"] 72 1 0.1 0.1 0.3 h_plus_time = np.arange(len(h_plus)) * h_plus_timeseries.deltaT + float(h_plus_timeseries.epoch) 85 1 0.0 0.0 0.0 return h_interp 84 print(msg)","title":"Profile the generation"},{"location":"old_notes/importance_reweighting_example/","text":"Importance reweighting example \"\"\" Script to test importance reweighting In this script we generate data according to y = m x + c + Normal(0, sigma) Then 1) Calculate the \"full\" posterior P(m, c | data) 2) Calculate the \"partial\" posterior P(m | data, c=0) (note c=0 in the injection) 3) Use importance reweighting to calculate P(m| data) from the \"partial\" results \"\"\" import matplotlib.pyplot as plt import numpy as np import bilby from scipy.special import logsumexp import tqdm np . random . seed ( 1234 ) outdir = '.' sampler = 'dynesty' npoints = 5000 def marginalized_log_likelihood_over_c ( m , likelihood ): \"\"\" Calculates L(data| m), marginalized over c Parameters ---------- m: float The fixed value of m at which to calculate the marginalized likelihood likelihood: bilby.core.likelihood.Likelihood instance Used to evaluate the log likelihood Note ---- Integration range chosen to cover the region of interest. Note, this neglects the normalization factors which don't count in the weight calculation. \"\"\" likelihood . parameters [ 'm' ] = m c_array = np . linspace ( - 0.2 , 0.2 , 100 ) integrand = [] for c in c_array : likelihood . parameters [ 'c' ] = c integrand . append ( likelihood . log_likelihood ()) return logsumexp ( integrand ) def model ( x , m , c ): return m * x + c # Injection parameters and create data m = 1 c = 0 sigma = 0.1 N = 100 x = np . linspace ( 0 , 1 , N ) y = model ( x , m , c ) + np . random . normal ( 0 , sigma , N ) likelihood = bilby . core . likelihood . GaussianLikelihood ( x , y , model ) # Run the full PE priors = dict () priors [ 'm' ] = bilby . core . prior . Uniform ( 0 , 5 , 'm' ) priors [ 'c' ] = bilby . core . prior . Uniform ( - 2 , 2 , 'c' ) priors [ 'sigma' ] = sigma full_result = bilby . run_sampler ( likelihood = likelihood , priors = priors , sampler = sampler , npoints = npoints , outdir = outdir , label = 'full' ) full_result . plot_corner () # Run the constrained PE priors = dict () priors [ 'm' ] = bilby . core . prior . Uniform ( 0 , 5 , 'm' ) priors [ 'c' ] = 0 priors [ 'sigma' ] = sigma partial_result = bilby . run_sampler ( likelihood = likelihood , priors = priors , sampler = sampler , npoints = npoints , outdir = outdir , label = 'partial' ) partial_result . plot_corner () # Pull out the uniformly-weighted samples from the full and partial runs full_m_samples = full_result . posterior . m . values partial_m_samples = partial_result . posterior . m . values # Calculate primed likelihood log_likelihood_prime = [] for m in tqdm . tqdm ( partial_m_samples ): log_likelihood_prime . append ( marginalized_log_likelihood_over_c ( m , likelihood )) log_likelihood_prime = np . array ( log_likelihood_prime ) # Calculate p, the normalized probably for each sample in partial_m_samples log_likelihood = partial_result . posterior . log_likelihood . values weights = log_likelihood_prime - log_likelihood p = np . exp ( weights ) p /= np . sum ( p ) # Reweight to get corrected samples reweight_samples = np . random . choice ( partial_m_samples , size = 30000 , p = p ) # Plot bins = np . linspace ( 0.9 , 1.1 , 50 ) fig , ax = plt . subplots () ax . hist ( full_m_samples , bins = bins , density = True , label = \"full\" , histtype = 'step' , linewidth = 2.5 ) ax . hist ( partial_m_samples , bins = bins , density = True , alpha = 0.5 , label = \"partial\" ) ax . hist ( reweight_samples , bins = bins , density = True , alpha = 0.5 , label = \"resampled\" ) ax . legend () ax . set_xlabel ( \"m\" ) plt . savefig ( \"posterior\" ) Having run this script, we obtain three images. First, the full posterior Second, the posterior when fixing c=0 Finally, the rewighted posterior from fixed case","title":"Importance reweighting example"},{"location":"old_notes/importance_reweighting_example/#importance-reweighting-example","text":"\"\"\" Script to test importance reweighting In this script we generate data according to y = m x + c + Normal(0, sigma) Then 1) Calculate the \"full\" posterior P(m, c | data) 2) Calculate the \"partial\" posterior P(m | data, c=0) (note c=0 in the injection) 3) Use importance reweighting to calculate P(m| data) from the \"partial\" results \"\"\" import matplotlib.pyplot as plt import numpy as np import bilby from scipy.special import logsumexp import tqdm np . random . seed ( 1234 ) outdir = '.' sampler = 'dynesty' npoints = 5000 def marginalized_log_likelihood_over_c ( m , likelihood ): \"\"\" Calculates L(data| m), marginalized over c Parameters ---------- m: float The fixed value of m at which to calculate the marginalized likelihood likelihood: bilby.core.likelihood.Likelihood instance Used to evaluate the log likelihood Note ---- Integration range chosen to cover the region of interest. Note, this neglects the normalization factors which don't count in the weight calculation. \"\"\" likelihood . parameters [ 'm' ] = m c_array = np . linspace ( - 0.2 , 0.2 , 100 ) integrand = [] for c in c_array : likelihood . parameters [ 'c' ] = c integrand . append ( likelihood . log_likelihood ()) return logsumexp ( integrand ) def model ( x , m , c ): return m * x + c # Injection parameters and create data m = 1 c = 0 sigma = 0.1 N = 100 x = np . linspace ( 0 , 1 , N ) y = model ( x , m , c ) + np . random . normal ( 0 , sigma , N ) likelihood = bilby . core . likelihood . GaussianLikelihood ( x , y , model ) # Run the full PE priors = dict () priors [ 'm' ] = bilby . core . prior . Uniform ( 0 , 5 , 'm' ) priors [ 'c' ] = bilby . core . prior . Uniform ( - 2 , 2 , 'c' ) priors [ 'sigma' ] = sigma full_result = bilby . run_sampler ( likelihood = likelihood , priors = priors , sampler = sampler , npoints = npoints , outdir = outdir , label = 'full' ) full_result . plot_corner () # Run the constrained PE priors = dict () priors [ 'm' ] = bilby . core . prior . Uniform ( 0 , 5 , 'm' ) priors [ 'c' ] = 0 priors [ 'sigma' ] = sigma partial_result = bilby . run_sampler ( likelihood = likelihood , priors = priors , sampler = sampler , npoints = npoints , outdir = outdir , label = 'partial' ) partial_result . plot_corner () # Pull out the uniformly-weighted samples from the full and partial runs full_m_samples = full_result . posterior . m . values partial_m_samples = partial_result . posterior . m . values # Calculate primed likelihood log_likelihood_prime = [] for m in tqdm . tqdm ( partial_m_samples ): log_likelihood_prime . append ( marginalized_log_likelihood_over_c ( m , likelihood )) log_likelihood_prime = np . array ( log_likelihood_prime ) # Calculate p, the normalized probably for each sample in partial_m_samples log_likelihood = partial_result . posterior . log_likelihood . values weights = log_likelihood_prime - log_likelihood p = np . exp ( weights ) p /= np . sum ( p ) # Reweight to get corrected samples reweight_samples = np . random . choice ( partial_m_samples , size = 30000 , p = p ) # Plot bins = np . linspace ( 0.9 , 1.1 , 50 ) fig , ax = plt . subplots () ax . hist ( full_m_samples , bins = bins , density = True , label = \"full\" , histtype = 'step' , linewidth = 2.5 ) ax . hist ( partial_m_samples , bins = bins , density = True , alpha = 0.5 , label = \"partial\" ) ax . hist ( reweight_samples , bins = bins , density = True , alpha = 0.5 , label = \"resampled\" ) ax . legend () ax . set_xlabel ( \"m\" ) plt . savefig ( \"posterior\" ) Having run this script, we obtain three images. First, the full posterior Second, the posterior when fixing c=0 Finally, the rewighted posterior from fixed case","title":"Importance reweighting example"}]}