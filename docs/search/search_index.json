{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Dr Gregory Ashton","text":""},{"location":"#about-me","title":"About me","text":"<p>I am a Senior Lecturer in Physics at Royal Holloway, University of London. I began my research career at the University of Southampton, working with Ian Jones and Reinhard Prix, where I completed my PhD in 2016 on Timing variations in neutron stars: models, inference and their implications for gravitational waves. Following this, I held a postdoc position at the Albert Einstein Institute, Hannover, before moving in 2018 to Monash University in Australia to work as an Assistant Lecturer with Paul Lasky. I then briefly joined the Institute for Cosmology and Gravitation (ICG) at the University of Portsmouth, working with Laura Nuttall as a Research Fellow before taking up my current post. My research interest is the relativistic astrophysics of neutron stars and black holes. I am a member of the LIGO Scientific Collaboration and co-chair the Collaboration's largest and most active observational science group, the Compact Binary Group.</p> <ul> <li>Contact: Send me an email at gregory.ashton@ligo.org</li> <li>Publications: See this ADS library or my RHUL Pure portal</li> </ul>"},{"location":"#news","title":"News","text":"<ul> <li>26/08/2025 It has been a very busy summer, but today the GWTC-4.0 introduction, methods, and results papers landed on arXiv. Below I have added a plot I mocked up for EPO showing the component masses:</li> </ul>      The mass-mass plot for all events in GWTC-4.0, showing new events added from O4a data (blue) alongside O1-O3 observations (grey). Each marker is the median of the component mass posterior distribution from either an equally mixed set of waveforms or a preferred waveform. The size of the marker corresponds to the network signal-to-noise ratio with an arbitrary scaling. Dashed lines indicate curves of constant total mass M, while dotted lines indicate curves of constant mass ratio q.    <ul> <li>21/05/2025 RHUL is hosting SPINS-UK 2025</li> <li>19/03/2025 RHUL PhD student Ann Malz is co-hosting the SEPNet student-led conference Exploring the Diversity of Astronomy and Astrophysics Research within GRADnet</li> <li>01/02/2025 Through January and Febuary, we have been observing with RHUL's astrodom</li> </ul>      RHUL BSc and MSc students using RHUL's astrodome to observe NGC 744 to find variable stars         The RHUL astrodome    <ul> <li>17/12/2024 PhD student Ann Malz posted her paper to the arXiv: Classification uncertainty for transient gravitational-wave noise artefacts with optimised conformal prediction</li> <li> <p>13/12/2024 On the final day of a long semester I gave a talk at the ODSL AI4Science Seminar series on Calibrating gravitational-wave search algorithms with conformal prediction</p> </li> <li> <p>23/09/2024 Mattia's paper was published in Phys Rev D</p> </li> <li> <p>23/09/2024 This week is the LVK meeting in Barcelona!</p> </li> </ul> <p> </p>      The conference venue: The Hotel Barcalona with views over Pla\u00e7a Catalunya     <p> </p>      Mattia Emma presenting recently-published work on Comparing advanced-era interferometric gravitational-wave detector network configurations: sky localization and source properties .    <p> </p>      Ann Malz presenting her work on the application of Conformal Prediction to the GravitySpy glitch classification algorithm.    <ul> <li> <p>11/09/2024 At SPINS-2024 Dr Adriana Dias gave a talk on observing step changes in the periodicities of PSR B1828-11.</p> </li> <li> <p>01/07/2024 Today I was invited to talk at the Gravitational Waves Astrophysics Conference 2024 on \"An overview of observational results from the LVK detectors up to the fourth observing run\". This was a for-the-LVK talk.</p> </li> <li> <p>18/06/2024 Calibrating gravitational-wave search algorithms with conformal prediction was published in PRD.</p> </li> <li> <p>15/06/2024 Today I lead the Physics department's contributions to the RHUL Science Festival, below is the \"Big Bang\" explosion from my talk on \"Colliding black holes and what they can tell us about quantum gravity\".</p> </li> </ul> <p></p> A gif of the Big Bang explosion: credit to Andrew Casey for getting the cryotechnics correct and not exploding the bin as well! <ul> <li> <p>05/06/2024 Today I gave a talk to the Cardiff Gravity Exploration Institute on \"Searching and characterizing compact binary coalescence signals: challenges and solutions in real data\"</p> </li> <li> <p>29/05/2024 This afternoon I presented at our local seminar on Quantifying Uncertainty with Conformal Prediction.</p> </li> <li> <p>15/05/2024 I visited the Keele Data Science and Astrophysics group and gave a seminar \"Gravitational Wave Astronomy: from interferometric strain to astrophysics\"</p> </li> <li> <p>25/04/2025  New paper day: Work led by PhD strudent Mattia Emma on Comparing advanced-era interferometric gravitational-wave detector network configurations: sky localization and source properties. We used a lot of CPU hours on this thanks to Oracle for Research for their support. This work also had contributions from Tiago Fernandes de Nobrega, an undergraduate who began the work on low-frequency analyses last year. </p> </li> <li> <p>05/04/2024 This week, I am visiting the IJCLab and gave the invited colloqium</p> </li> </ul> <p></p> A photo of the presentation at the IJCLab <ul> <li> <p>18/03/2024 Ronaldas' paper is now published in Phys Rev D! </p> </li> <li> <p>13/03/2024 Mattia ran a IOP-supported student conference Physics is You!</p> </li> <li> <p>29/02/2024 New paper day: Calibrating gravitational-wave search algorithms with conformal prediction is on the arXiv and submitted to Phys Rev D.</p> </li> <li> <p>17/11/2023 New paper day: Work led by Ronaldas Macas Revisiting GW200129 with machine learning noise mitigation: it is (still) precessing</p> </li> <li> <p>07/Aug/2023 We hosted an In2Science student Pavan at RHUL. Pavan built the first iteration of the Duck Detector, a concept idea for a gravitational-wave outreach project (see image below). Overall the project was a success and afterwards Pavan said \"He's relentless and passionate about physics and astronomy. He's handled the work experience like an absolute professional rightfully so. My experience couldn't have been any better, thanks to my mentor and his passion towards the subject and his willingness to outreach and teach the ways to young children. The best someone could expect.\" which I of course was very happy to hear.</p> </li> </ul> <p> </p> Image of the v1 Duck Detector <ul> <li> <p>18/May/2023 I gave a colloqium on Glitch-robust gravitational wave astronomy at Los Alamos (virtually).</p> </li> <li> <p>15/May/2023 I co-organised and helped to run the 6th GWOSC Open Data Workshop. Below is a map of all the study hubs!</p> </li> </ul> <p></p> The map of study hubs for the sixth GWODW <ul> <li> <p>11/May/2023 I gave a colloqium at the Institute for Cosmology and Gravitation, University of Portsmouth. I talked about Glitch-robust gravitational-wave astronomy!</p> </li> <li> <p>05/Apr/2023 Double new paper day! Jack Heinzel submitted Inferring the Astrophysical Population of Gravitational Wave Sources in the Presence of Noise Transients to MNRAS for review while Rowina Nathan submitted Improving pulsar-timing solutions through dynamic pulse fitting also to MNRAS for review. Both of these are excellent PhD students I have had the good forture to help with their project.</p> </li> <li> <p>06/Feb/2023 My paper Gaussian Processes for Glitch-robust Gravitational-wave Astronomy was published in MNRAS.</p> </li> <li> <p>06/Dec/2022 I was invited to speak on Key results from ground-based gravitational-wave detectors at the GWPAW 2022 meeting.</p> </li> <li> <p>01/Sep/2022 PhD student Mattia Emma started in my group!</p> </li> <li> <p>13/Jul/2022 Our paper Parameterised population models of transient non-Gaussian noise in the LIGO gravitational-wave detectors has been published in CQG.</p> </li> <li> <p>11/Jul/2022 I led the organisation of the Gravitational-wave astronomy parallel session at NAM2022. We where over subscribed by a factor of 2 on talks and the room was full too. Really exciting to see the breadth and talent of people working on GWs in the UK. Thanks to all the other organisers who helped make it happen.</p> </li> </ul> <p></p> Image of the packed room at NAM 2022  <ul> <li> <p>04/Jul/2022 My paper with Tim Dietrich The use of hypermodels to understand binary neutron star collisions was published in Nature Astronomy.</p> </li> <li> <p>01/Jun/2022 Our Nested Sampling primer was published in Nature Reviews. While I am first author, you should realise this is because \"A\" is the first letter of the alphabet! Andrew Fowlie led the effort and I am deeply indepted to him as I learned a lot. My contribution, along with Matt Pitkin and John Veitch was to the gravitational-wave application section.</p> </li> <li> <p>25/May/2022 Today, I joined the smallpeice trust and RHUL's Girl's Into Astrophysics event!</p> </li> <li> <p>29/Apr/2022 It is a new paper day! GWCloud hit the arXiv which details the inner workings of searchable repository for the creation and curation of gravitational-wave inference results. This project started back in 2018 IIRC with several ADACS applications by Paul Lasky. It has been a pleasure to work on and I look forward to the future of the project.</p> </li> <li> <p>27/Apr/2022 Today I had the pleasure of examing a PhD student thesis. I'll delay in giving the name until it is all official. But, they defended very well and can now proudly call themselves Docter!</p> </li> <li> <p>06/Apr/2022 I was a judge for the best Student Prize at BritGrav 2022. The conference consisted of two days of talks from students/postdocs and served as a great display of the exciting science done by scientists in the UK. The quality of talks was fantastic and it was hard to pick between them, but in the end Lucy Thomas won the best talk prize with Marion Cromb and Elsa Teixeira as runners up. Congratulations to them all.</p> </li> <li> <p>01/Feb/2022 I am an Award Lead for the Alan Turing Network Development Award. I'm looking forward to using this to develop some interdisciplinary projects.</p> </li> <li> <p>01/Dec/2021 Today I joined the SEPNet workshop Equality, Diversity &amp; Inclusion \u2013 Revisiting the leaky pipeline \u2013 short-term contracts and career planning. You can find my slides here.</p> </li> <li>18/Nov/2021 I presented at the Banff IRS workshop Detection and Analysis of Gravitational Waves in the era of Multi-Messenger Astronomy: From Mathematical Modelling to Machine Learning. You can find a recording here and my slides here.  </li> <li> <p>17/Nov/2021 With Tim Dietrich, we put out a new preprint Understanding binary neutron star collisions with hypermodels. This one shows some tentative evidence for waveform systematics in a BNS.</p> </li> <li> <p>01/Nov/2021 Today I joined Royal Holloway as a Lecturer in Physics!</p> </li> <li> <p>26/Oct/2021 A paper led by the ICG's Simone Mozzon Does non-stationary noise in LIGO and Virgo affect the estimation of H0? hit the arXiv today.</p> </li> <li> <p>07/Oct/2021 New paper day! Parameterised population models of transient non-Gaussian noise in the LIGO gravitational-wave detectors. This work has taken nearly 2 years from the initial conception, a huge amount of computing, and lots of thinking. I really enjoyed getting into a new aspect of GW astronomy, namely the characterisation of the detector.</p> </li> <li> <p>17/Sep/2021 My daughter Ada was born at 4am. Very happy, excited, and tired!</p> </li> <li> <p>29/Jul/2021 New paper led by PhD student Avi Vajpeyi on A search for intermediate-mass black holes mergers in the second LIGO--Virgo observing run with the Bayes Coherence Ratio</p> </li> <li> <p>14/Jul/2021 Today I helped out in Royal Holloway's Girls into Physics program run by the Small Piece trust where we did Python programming for science</p> </li> <li> <p>29/Jun/2021 The LVK collaboration published Observation of gravitational waves from two neutron star-black hole coalescences in ApJL. This was a really fun project to be involved in. That we can make a definitive statement about the nature of the secondary (light-mass) object with a counterpart is testament to our understanding of controlling systematic uncertainty.</p> </li> <li> <p>17/Jun/2021 We released a new paper: Bilby-MCMC: An MCMC sampler for gravitational-wave inference. This was really fun as I got to write a sampler from scratch and remind myself of all the gory details.</p> </li> <li> <p>16/Jun/2021 I helped out with the Royal Holloway Particle Physics Masterclass introducing python programming.</p> </li> <li> <p>11/May/2021 New paper with PhD student Zhi-Qiang You: Optimized localization for gravitational-waves from merging binaries submitted to MNRAS.</p> </li> <li> <p>10/May/2021 The Gravitational Wave Open Data Workshop #4 kicked off today on gather.town.</p> </li> <li> <p>12/Apr/2021 Today I was elected as co-chair of the LIGO collaboration Compact Binary Coalescence (CBC) group. I join Chad Hanna and Walter Del Pozzo and look forward to helping coordinate the discovery and analysis of signals from colliding black holes and neutron stars.</p> </li> <li> <p>08/Apr/2021 Today I'm taking part in Royal Holloway's Astrophysics Residential for 2021. I'm covering an introduction to programming and how to calculate Pi using random numbers.</p> </li> <li> <p>12/Mar/2021 My students successfully nominated me for a You\u2019re Valued Award at Royal Holloway. I'm proud to add some of their comments here: \u201cGreg is an amazing teacher. His enthusiastic approach to teaching maths to us foundation year students has made us enthusiastic as well. he is very friendly and approachable and when we need help he very patiently helps explain everything to us. He is dedicated to make us understand all  the concepts he teaches and help us learn to our best capacity.\u201d and \u201cGreg always focuses on our individual worries and makes sure we understand before moving on. He's very good at teaching and we're happy to have him as our maths professor :)\u201d.</p> </li> <li> <p>08/Feb/2021 I presented Flickering of the Vela pulsar to the CAMK journal club.</p> </li> <li> <p>28/Jan/2021 My PhD student Nikhil Sarin passed his pre-submission milestone. It has been a pleasure to watch Nik develop into a great researcher - his next employer will be lucky to have him.</p> </li> <li> <p>27/Jan/2021 New paper on arXiv! Work led by David Keitel \"PyFstat: a Python package for continuous gravitational-wave data analysis\" which updates the latest on the PyFstat package for Continuous-Wave analyses.</p> </li> <li> <p>14/Jan/2021 New paper! Work led by Eric Burns \"Identification of a Local Sample of Gamma-Ray Bursts Consistent with a Magnetar Giant Flare Origin\" hits the arXiv.</p> </li> <li> <p>11/Jan/2021 Semester 2 gets underway at RHUL. I'm teaching FY1006 Mathematics II to 140 students in online mode for the foreseable future.</p> </li> <li> <p>07/Jan/2021 I gave a talk titled \"The deepening mystery of the Vela radio-pulsar glitch\" at the Department of Physics, Bar-Ilan University on our recent Vela paper.</p> </li> <li> <p>04/Dec/2020 I gave an improptu talk to the University of Southampton's astrophysics group on our recent Vela paper.</p> </li> <li> <p>16/Nov/2020 New paper \"Flickering of the Vela pulsar during its 2016 glitch\" on the arXiv.</p> </li> <li> <p>10/Nov/2020 I was awarded the 2020 USERN Physical and Chemical Sciences prize.</p> </li> <li> <p>04/Nov/2020 I gave a presentation to Royal Holloway's Physics group \"Turning Wiggles into Science\"</p> </li> </ul>"},{"location":"#gallery","title":"Gallery","text":""},{"location":"group/","title":"Group","text":"<p>On this page, I list students that I have supervised and their projects. If you would like to contact a student, please reach out and I would be more than happy to put you in touch.</p>"},{"location":"group/#postdoctoral-researchers","title":"Postdoctoral researchers","text":""},{"location":"group/#dr-adriana-dias-2023-2024","title":"Dr Adriana Dias (2023-2024)","text":"<p>I am an Research Fellow working with Greg Ashton on timing variations of radio pulsars, with a particular interest on the glitch phenomena observed in the PSR B1828-11 radio pulsar. My background is in Particle Physics, having obtained a Masters degree in 2019 and a PhD degree in 2023 at Royal Holloway, University of London. My Masters project, Calibration of a High-Pressure Time Projection Chamber, involved working on the hardware and calibration systems of a High-Pressure Time Projection Chamber, to be used in Neutrino Physics Research. I was directly involved in the calibration and construction of this detector. My PhD project, Detector Development for Particle Physics and Applications to Environmental Monitoring, involved the development of a novel device, called PlomBOX, employing a CMOS sensor and lead-sensing bacteria to assay lead in drinking water, up to the World Health Organisation\u2019s upper limit of 10 ppb. I was involved in the development of the data acquisition and slow control tools and developed the analysis interface used by the PlomBOX.</p> <p>Outside research, I am passionate about making physics available to people from working class backgrounds and as such I help lead a Working Class in Physics Forum at Royal Holloway. I am an avid gamer, love to read and enjoy nature walks.</p>"},{"location":"group/#phd-students-supervision","title":"PhD students supervision","text":""},{"location":"group/#ann-malz","title":"Ann Malz","text":"<ul> <li>Start date: 2023</li> <li>Thesis title: TBC</li> </ul> <p>I am a PhD student in astrophysics, working with Greg Ashton and Nicolo Colombo on using machine learning for gravitational wave analysis. I am currently working on applying conformal prediction to determine uncertainties. When making use of machine learning, for example applied to gravitational wave parameter estimation, quantifying the uncertainty of how accurate the estimate is is essential. Conformal prediction provides a method to determine this uncertainty for any point prediction algorithm. </p> <p>My background is in theoretical physics and astrophysics, with a Masters degree from the University of Glasgow. In my masters project, working with John Veitch, I applied machine learning in the form of normalising flows to model and hence remove glitches from gravitational wave data, with the aim to improve the Bayesian inference based parameter estimation in the presence of glitches. </p> <p>Besides research, I enjoy volunteering with projects promoting science and engaging young people in STEM, and have been involved in various projects with the Swedish Federation of Young Scientists, such as organising Rays Research Academy for Young Scientists. In my free time I also enjoy a variety of outdoor activities as well as good literature.</p> <p></p>"},{"location":"group/#sean-hibbit","title":"Sean Hibbit","text":"<ul> <li>Start date: 2023</li> <li>Thesis title: TBC</li> </ul> <p>I'm currently a PhD student at Royal Holloway, working with Xavier Rojas and Greg Ashton who is my second supervisor. My work is focused on an idea to further the Weber bar resonance bar concept using Low-Temperature superdluid optomechanics. I completed my MSci at Royal Holloway, my dissertation was on modelling many body quantum systems; this also included a research review on quantum computers and associated algorithms.</p> <p>My previous projects include</p> <ul> <li>Creating a computer model, to simulate many-body quantum systems and explore the Fermi-Dirac Distribution with Gr\u00e9goire Ithier.</li> <li>Designing and building a successful low-temperature experiment, currently used to introduce undergraduate to low temperature physics. This was under L. V. Levitin.</li> <li>Designing and creating software in python to manage a vapor deposition nano-fabrication tool for Phil Meeson.</li> </ul> <p></p>"},{"location":"group/#mattia-emma","title":"Mattia Emma","text":"<ul> <li>Start date: 2022</li> <li>Thesis title: TBC</li> </ul> <p>I am a PhD student in gravitational wave astronomy under the supervision of Dr. Greg Ashton. My research portfolio spans various facets of gravitational wave physics, where I actively contribute as a member of the Laser Interferometer Gravitational-Wave Observatory (LIGO). One significant aspect of my work involves gravitational wave data analysis, where I delve into the intricacies of parameter estimation. Thus far I have been looking into the problems arising from the approximations used in our analysis with the increasing amount of detected signals, and into the benefits introduced by using more detectors for the analysis and their dependence on the performance of the single detectors.</p> <p>Following my MSc in Astrophysics in Potsdam, where I focused on Numerical Relativity simulations, I am still collaborating with Prof. Dr.  Tim Deatrich and Dr. Violetta Sagun, specifically exploring dark matter-admixed neutron stars and their detectability through our current and future ground-based detectors. I have completed my BSc in Physics at La Sapienza University in Rome, Italy, where my thesis on the constraints on the tidal deformability of neutron stars through gravitational waves was supervised by Prof. Dr. Francesco Pannarale.</p> <p>Beyond the academic realm, I am passionately committed to fostering equal access to education globally. As a Cumberland Lodge Fellow in the 2023-2025 cohort, I actively participate in initiatives aimed at raising awareness of societal issues and trying to find solutions through dialogue. In this pursuit, in 2019 I founded a non-governmental organization dedicated to combating educational disparities, with a specific focus on enabling girls from underprivileged backgrounds to access secondary education. Additionally, I am a basketball player for the University team and my interests extend to the realms of African literature, history, and cooperative endeavors.</p> <p></p>"},{"location":"group/#nikhil-sarin","title":"Nikhil Sarin","text":"<ul> <li>Start date: 2018</li> <li>Graduated: 10/2021</li> <li>Thesis title: \"The observational consequences of neutron star postmerger remnants\"</li> <li>Personal website: nikhil-sarin.github.io</li> </ul> <p>I co-supervised Nikhil along with Paul Lasky while an Assistant Lecturer at Monash University. Nikhil had a wide range of interests and was a wonderful student to supervise, always pushing me into parts of astronomy I hadn't yet ventured and studying the literature in depth as he went. Nikhil is now a NORDITA Fellow and overall excellent scientist.</p>"},{"location":"group/#phd-student-support","title":"PhD student support","text":"<p>Below, I list students to which I have contributed to their supervision.</p> <ul> <li>Zhi-Qiang You</li> <li>Rowina Nathan</li> <li>Simone Mozzon</li> </ul>"},{"location":"group/#summer-project-students","title":"Summer project students","text":"<ul> <li>2025<ul> <li>TBC</li> </ul> </li> <li> <p>2024</p> <ul> <li>Julianna Ostrovska Joined us (again!) for anotehr 6-week RHUL summer placement. This time, Julianna investigated Simulation Based Inference approaches and compared them with likelihood-based stochastic samplers. (Co-supervised by Mattia Emma)</li> <li>Kristina (Kika) Latkoczyova (Royal Astonomical Society Bursary). Kika joined us and developed a demonstration of using a supervised ML approach for gravitational-wave search. (Co-supervised by Ann Malz)</li> <li>Lucie Robbins joined us and continued the investigation of the periodic modulations in PSR B1828-11.</li> </ul> </li> <li> <p>2023</p> <ul> <li>Julianna Ostrovska joined us for a 6-week RHUL summer placement studying the periodic modulations of the radio pulsar B1828-11</li> <li>Tiago Fernandes De Nobrega joined us for a 6-week RHUL summer placement on \"Gravitational Wave Detectors: Exploring the Configuration to Optimise Source Parameter Interference\"</li> <li>A-level student Pavan joined us for the 2-week project funded by In2Science to build a \"duck detector\" to simulate the triangulation of gravitational-wave detectors</li> </ul> </li> <li> <p>2022</p> <ul> <li>Floyd Hyatt joined us for a 6-week summer placement on optimisation methods for gravitational-wave parameter estimation</li> <li>Rahmah Mackie joined us for a 3-week summer placement helping to develop new teaching materials for our \"Advanced Skills\" module</li> </ul> </li> </ul>"},{"location":"help/","title":"Help","text":"<p>This page contains some helpful hints and tips that I've leared through my career. I don't claim these to be perfect solutions, you have suggestions, please feel free to email me.</p>"},{"location":"help/#writing-a-paperdocument","title":"Writing a paper/document","text":"<p>Writing scientific papers is a central part of life as a scientist. Like most things, investing a bit of time/money can make the process a little easier and allow you to focus on writing rather than the boring admin that inevitably goes with it. Here are a few ad-hoc things I've picked up which may help:</p> <ol> <li> <p>Choose your medium. There are many ways to write a paper. You can use <code>latex</code> directly on your computer (i.e. write a <code>.tex</code> file and compile it with <code>pdflatex</code>); you can use cloud-based latex solutions like Overleaf (see below); or you could use Word/Google Docs. All of these are great and have their place. Personally, I use <code>latex</code> directly when I'm working on a paper with more than 10 contributors and we need to use <code>git</code> to track changes and merges. I use Overleaf for most papers I write, and I use Word/Google Docs for pretty much anything which does not required images or equations.</p> </li> <li> <p>Use the tools. There are a number of tools which can simplify your life. Taking the pain out of compilling latex, to automatically identifying and fixing spelling, gramatical, and typographic errors. As someone who has some mild dyslexia (and did not pay enough attention at school), I've found these invaluable. Given the availability of the checkers (most are free), it demonstrates real laziness to submit journal articles with typos (something I have definitely done). Here are my favourits:</p> <ul> <li>Overleaf: this is a cloud-based latex editor/compiler. For projects where you have a handful of collaborators, the free version works great! Make sure to use the built-in spelling correction. Overlead has an incredible ability to ignore Latex errors and still produce output. This is really useful during the the writing stage. But please, for the love of all that is good in this world, fix those errors eventually. They will result in broken arXiv compillations and usually many some aspect of your document is missing.</li> <li>Grammarly: there is a paid version which provides more in-depthanalysis. In my mind this is worth every penny if you are submitting grants/fellowships where a single typo could upset a reviewer (they are fickle beasts). But, the free version is also great. You can use this for Email, Word documents, and Overleaf (see this S/O page).</li> <li>codespell: this is great for when you are forced to work on a latex document without overleaf.</li> </ul> </li> <li> <p>Use latex macros for results: If your paper contains results from any sort of data analysis, it is likely you'll need to communicate some numbers (e.g. the mass of a star). Please, save yourself much heartache, do not hardcode those numbers into the paper. Instead, create a macro:     <pre><code>\\newcommand{\\massofstar}{32.5}\n</code></pre>     then anywhere you refer to the mass, you simply write <code>\\massofstar</code>. Better yet, have your analysis script write the <code>newcommand</code> lines for you in a file <code>macros.tex</code>, then add <code>\\include{macros}</code> in your latex document. Now, when you inevitably find a bug in your code the day before submission, you simply rerun and replace <code>macos.tex</code>. This also allows you to <code>diff</code> between the new and old <code>macros.tex</code> to see what changed! Finally, avoid editing the macros by hand. If you need to round the numbers, do this in the analysis script (this usually takes some thinking, but the uncertainties package can help here).</p> </li> <li> <p>Use latex macros for everything: Another common headache the day before submission is that you realise you have used <code>\\theta</code> to mean to different things. Doh! Or, perhaps your collaborator doesn't like you calling the dimensionaless spin of the black hole <code>\\chi</code> and prefers <code>a_1</code>. To avoid the dangers of a last-minute Ctrl+F and replace, when you start writing never use symboles directly unless your are super super confident they will not change. Instead, define macros like     <pre><code>\\newcommand{\\dimensionalspinmaginitude}{a_1}\n</code></pre>     Being verbose is good here: it means you and your collaborators know that you mean the dimensionless spin magnitude, and not something else! You can also do this for things like software names where the formatting is usually journal specific. Add macros like     <pre><code>\\newcommand{\\bilby}{\\textsc{Bilby}}\n</code></pre>     Then when you realise the journal prefers <code>texttt</code>, you change one thing.</p> </li> <li> <p>Organise your bibtex files. Pick a convention for the keys (I like <code>AUTHOR_YEAR</code>, but oftentimes people like to use the ADS/Inspire key) and stick with it. This avoids repeated entries. You can use online tools like this to tidy up a messy <code>bib</code> file. Both NASA ADS and InspireHEP have nice utilisies to generate bibtex files (see, e.g. here and click cite here). They also allow you to pick the format as well. Personally, I'm not a fan of maintaining a single huge <code>bib</code> file: I write a new one for each paper.</p> </li> <li> <p>Learn some grammar: Honestly, I could not of cared less about grammar while at school. Only during my final year as an undergraduate, when we had to write a 10-page report, I realised that actually, communicating what I had learned was maybe a little bit important. While Grammarly and other tools can help, it is good to learn why they are making changes. Equally, if you have a collaborator who edits the text, ask them why and don't be ashamed. Better to learn later than never.</p> </li> <li> <p>Be consistent in your style: English is a flexible language, but when reading a scientific article, a consistent style of writing avoids distracting the reader. For me, I found \"The Elements of Style\" by Strunk and White to be hugely useful. I don't agree with all of it, but I find it useful to have a single point of reference to consult when I'm unsure. I don't always follow the advice (often you need the passive voice), but I try to maintain it on the whole. There is also an automated checker in perl which you can use to identify problem areas.</p> </li> </ol>"},{"location":"help/#my-scriptjobtest-is-not-working-what-should-i-do","title":"My script/job/test is not working, what should I do?","text":"<p>This is a very common problem. For anyone involved in scientific research which relies on an element of programming, you will no doubt find that, at some point you are faced with a situation where \"it isn't doing what it should be!\" or \"it won't run!\". For those of us, like me, who did not receive formal training on software development, this can be daunting. Your supervisor or colleagues may expect you to solve it without help, what do you do? It might be tempting to either give up or ask for someone else to fix it. But, before you do either of things try to work through this list of steps:</p> <ol> <li> <p>Breath: it can be frustrating so first things first, try to take a step back. Grab a cup of tea/coffee/water and let your head clear.</p> </li> <li> <p>Read: now, try reading both your script and the error message (in Python, this is called the <code>TraceBack</code>). The output messages might be quite long, but try scanning through it and find:</p> <ul> <li>Which line of your script causes the error? What is it trying to do?</li> <li>What is the error message (if any) from the output? Does it tell you which program caused the issue? You may find this guide useful in understanding the <code>TraceBack</code></li> </ul> </li> <li> <p>Check: Now check out what else is going on:</p> <ul> <li>How long does it take to fail, does it take a long time to fail or straight away?</li> <li>What else is happening with your computer at the time? Does the memory/CPU usage of the program spike?</li> <li>If your script relies on software, what version are you using? Is it up to date, did you update it recently?</li> <li>Have you backed up your script, can you roll back to an earlier version to see if it works?</li> </ul> </li> <li> <p>Investigate: hopefully by reading the scripts and logs, you might have an inkling about what is going on. Try to test it out, perhaps remove the line that is failing. What happens? Do you get a new error message? The goal here is to build some intuition about what the error is and the cause.</p> </li> <li> <p>Research: Now that you have an idea about what is going on, it is time to research. A few places that are worth searching:</p> <ul> <li>Google/StackOverflow: An easy first step is to simple search the error message on the internet. But, be warned: copy and pasting solutions is usually a recipe for disaster. Instead, read the solutions and comments to help you understand how they solve the problem.</li> <li>Documentation: If the problem seems to be in a specific piece of software, try to find the documentation and scan it. Do they have a guide on troubleshooting, a place where they welcome questions?</li> <li>Source code: If you have identified which line is failing in any underlying software, find the source code (if it is open source). Read it over. It may well be that you have found a bug: this is a great oppotunity to contribute. If you can see how to fix it, try editing the source code, this may require you to install the software from the source code to test your fix. If your fix is successful, consider opening a Merge Request/Pull Request to fix the code.</li> </ul> </li> <li> <p>Identify who you will ask for help: If you have run through the steps above and things are still not working, you are now ready to ask for help. First, you need to identify who you will ask</p> <ul> <li>If the problem is in a specific piece of software, does that software have a guide for getting help?</li> <li>If you are working with a colleague/supervisor, they may be the right persont to contact now.</li> <li>If your questions are more general, try StackOverflow or look for local programming support groups (e.g. at your University).</li> </ul> </li> <li> <p>Ask for help: Now you know who to contact, you should write a message to them. Keep in mind the following:</p> <ul> <li>Keep it concise. Avoid long paragraphs and lots of background, focus on what they need to know to help you.</li> <li>Include a Minimum Working Example. This will help them reproduce your problem. In making it you may even solve the problem yourself!</li> <li>Include a description of your investigation and research: they will need to know all the details (i.e. versions of software, any oddities)</li> <li>Tell them what you have tried already. This will make sure they don't repeat the same investigations you have undertaken and help them to help you.</li> <li>If relevant, tell them why you are contacting them. If they have offered support, thank them for it! In some cases, what may appear to be a \"missing feature\" is in fact a large research project. If you need this feature, you may consider offering them an acknowledgement or even co-authorship on any subsequent publications. After all of this, send your message. Have some patience, they may be overwhelmed by other tasks when you email. Feel free to email them again after a week or so if you haven't got an answer. They may have simply forgotten!</li> </ul> </li> </ol>"},{"location":"help/#getting-started-in-the-ligo-collaboration","title":"Getting started in the LIGO Collaboration","text":"<p>There is a guide to getting started in the Collaboration, you can find this here. This contains a wealth of information. Here, I'm just listing a few other things I find useful.</p> <ol> <li> <p>If you use the Google chrome browser, you can use a Profile associated with your LIGO account. This creates a separate browser window which, by default, logs in to things with your LIGO credentials. This greatly reduced the amount of clicking between different Google logins I have to do when following links.</p> </li> <li> <p>Want to email someone in the collaboration, but not sure of their address? You can head to https://contacts.google.com/ and log in with your LIGO Credentials. This will provide you with all the <code>albert.einstein@ligo.org</code> addresses. With this, you can download it as a CSV and upload it to your email client. Now you have all of LIGO on tab completion!</p> </li> </ol>"},{"location":"help/#getting-a-usable-linux-environment-on-windows","title":"Getting a usable linux environment on Windows","text":"<p>The Windows OS used to be unusable for most scientists due to a host of compatibility issues. But, Windows now offers the \"Windows Subsystem for Linux\" which has changed that. Here are some links to useful guides</p> <ol> <li> <p>Installing Ubuntu in WSL. The default OS is Ubuntu, this has worked well for me. It is worth making sure you install WSL2.</p> </li> <li> <p>In theory, WSL now supports GUI apps natively (see here). I have yet to get this working, but it looks great.</p> </li> <li> <p>I have found that the Windows Terminal works really well. I actually prefer this to the native terminal in Ubuntu itself. There is a SO post here on getting it to launch WSL, but I think this may be a little out of date. You can add WSL by simply adding a new profile.</p> </li> </ol>"},{"location":"help/#getting-a-usable-python-environment-in-wsl-or-in-linux","title":"Getting a usable python environment (in WSL or in Linux)","text":"<p>I strongly recommend people use anaconda (AKA conda) to manage their python installation. The full instructions can be found here. If you are working in Ubuntu through WSL, follow the instructions for Linux/Debian. I'll provide a quick TLDR here:</p> <ol> <li> <p>Install the prerequisites</p> </li> <li> <p>Scroll to the bottom of this page and copy the link address for the Linux install, something like \"64-Bit (x86) Installer (581 MB)\" i.e. right click \"Download\" and hit Copy link address)</p> </li> <li> <p>Run <code>wget &lt;THE LINK ADDRESS</code> to download the file (Note: you can just download it from the browser, but if you are working in a virtual machine/on a cluster/in WSL you'll then need to move the file from your local machine to the target. Using <code>wget</code> directly skips this step).</p> </li> <li> <p>Install <code>anaconda</code> by running <code>bash ~/PATH/TO/FILE</code> where the <code>FILE</code> will be something like <code>Anaconda3-2020.02-Linux-x86_64.sh</code></p> </li> <li> <p>Finally, once you have installed <code>conda</code> and restarted, your command prompt will have start with <code>(base)</code>. This tells you that you are in the <code>base</code> conda environment. DO NOT INSTALL THINGS IN THE BASE ENVIRONMENT.</p> </li> <li> <p>Instead, follow this guide to create a new environment. For example, to create an environment called <code>testing</code> with <code>python 3.9</code>, run <code>conda create -n testing python=3.9</code>. Once complete, you'll need to activate the environment <code>conda activate testing</code>.</p> </li> <li> <p>While <code>conda</code> may seem like a headache (you will no doubt forget to activate the environment occasionally and wonder why nothing works!). This headache is minor compared to the headache which occurs when you need to install two different versions of <code>numpy</code> for two different projects. As such, use a new environment for each project you work on.</p> </li> </ol>"},{"location":"help/#using-git-to-track-initial-project-progress","title":"Using git to track initial project progress","text":"<p><code>git</code> is a super powerful tool. Most people have heard of <code>github</code> or <code>gitlab</code>, these are websites that store <code>git</code> repositories and allow people to collaborate in building software, to provide easy access, and to track developments in a meaningful way. However, <code>git</code> itself is simply a version control software and can be used without pushing the repository to <code>github</code> or <code>gitlab</code>. Here I describe a simple way I use <code>git</code> to track progress of projects during the initial conception.</p> <ol> <li> <p>Okay, so you have a cool idea for a project and you spent a day writing a hacky script which is the proof of concept. Great, now you want to expand it out, maybe add some bells and whistles.</p> </li> <li> <p>Before you do anything else. Put the script + anything else into a directory with a half sensible name, then run <code>git init</code>, <code>git add *</code>, and <code>git commit -m \"Initial commit adding everything\"</code>. Now you have a snapshot of the code.</p> </li> <li> <p>Now continue as you would anyway, perhaps adding a first bell then a whistle. But oh no! You broke the proof of concept... but how? Let's figure it out.. run <code>git diff</code> and you'll see the changes against your last working version. You spot the type fix it and then again add and commit everything.</p> </li> <li> <p>Continue this process. The more often you stop and commit things the better.</p> </li> <li> <p>At some point either you will figure out the project is a donut, in which case put it in the failed projects directory and move on. Or, you think it has legs. At this point you may choose to start a \"fresh\" git repo and try to make your commits more serious (no more curse words when you break it!). Or, you could just push your current repo to github. Who knows, when you win the Nobel prize for your ieda people may look back at those early commits to spot your genious! I think too often we wait far to long to start using proper project management tools. By allowing yourself to just use <code>git</code> locally (i.e. you don't need to push to a repo, set up a CI, and add testing), we gain a lot of benefits without the overhead. </p> </li> </ol>"},{"location":"notes/","title":"Notes","text":"<ul> <li>Notes from a workshop I gave at QMUL</li> <li>Hints and tips on working in Latex</li> <li>ImportError liblapack on MacOS</li> <li>Python scripting</li> <li>Projecting waveforms</li> <li>Splitting up merge requests</li> </ul>"},{"location":"notes/#derivations","title":"Derivations","text":"<ul> <li>Drawing samples from the Goodman &amp; Weare (2010) proposal distribution</li> </ul>"},{"location":"notes/#old-notes","title":"Old notes","text":"<ul> <li>11/Feb/20: Understanding PP plots</li> <li>27/Mar/19: Importance Reweighting Example</li> <li>08/Oct/19: Chirp time bounds</li> </ul>"},{"location":"projects/","title":"Projects","text":"<p>In this page, I collect various miscellaneous coding projects I have been involved in.</p> <ul> <li> <p>CBCFlow allows convenient and machine readable storage, communication, and retrieval of important metadata for CBC analyses of events</p> </li> <li> <p>Python for Science A set of workbooks to introduce python for scientific programming</p> </li> <li> <p>koala_html A package to quickly create html pages from collections of images</p> </li> <li> <p>kookaburra A python package for profile-domain timing of radio pulsars</p> </li> <li> <p>parallel_bilby A python package to leverage slurm-based HPC clusters. Enables scaling inference jobs up to many hundreds of cores.</p> </li> <li> <p>bilby_pipe A python package for automating the job of running multiple jobs on LIGO Data Grid clusters.</p> </li> <li> <p>bilby A python package providing a user   friendly interface to perform parameter estimation. It is primarily designed and built for inference of compact binary coalescence events in interferometric data, but it can also be used for more general problems.</p> </li> <li> <p>PyFstat A python   package containing various methods to run continuous gravitational wave   searches. Includes glitch-robust, MCMC-based, and transient work.</p> </li> <li> <p>Bayes Bimodal Test A simple   python module using the emcee MCMC software to perform a Bayesian model   comparison of bimodality.</p> </li> <li> <p>GitCheck: A python appindicator which   provides a visual check of the status of git repos. This builds on some of   the functionality of batchgit by   Max Hebditch.</p> </li> </ul> <ul> <li>GetTrainTimes: a command-line   tool to quickly get train times from the national rail (UK) website.</li> </ul> <ul> <li>pyweather: a command-line tool to   quickly get a visual (ASCII) forecast of the weather for (almost) any location.</li> </ul> <ul> <li> <p>Using the Google maps API to study average driving speeds around the    globe.    To see the results have a look here</p> </li> <li> <p>Printing latex elements:   This isn't so much a project as a useful script. Often in cleaning up latex   docs I trawl through the document searching for occurances of say   <code>includegraphics</code>. This script will simply print the elements (for example   in <code>\\label{eqn: an equation}</code> the element would be <code>eqn: an equation</code>) to the   command line. It takes multiple files and if unspecified will search for    proper tex files to use. It has default flags of <code>-f</code> to find figures and    <code>-l</code> to find labels, but you can specify whatever you want with <code>-o</code>. For   example</p> </li> </ul> <pre><code>    $ print_tex_elements somearticle.tex -o cite\n</code></pre> <p>might for example produce</p> <pre><code>Einstein1916\nNewton1675\n</code></pre>"},{"location":"science/","title":"Science","text":""},{"location":"science/#publications","title":"Publications","text":"<p>You can find all publications to which I have made significant contributions in this ADS library.</p>"},{"location":"science/#workshops","title":"Workshops","text":"<ul> <li> <p>June 2023 A Taste of Physics and Astronomy - RH23016. Lead organiser: 30 A-level students given a taste of University life, including lectures, admission discussions, and acitivities across the department.</p> </li> <li> <p>May 2023 GWOSC ODW #6. Co-organiser: a hybrid workshop with 2000+ people enrolled, 22 study local study hubs, and 300 active participants. I ran a local study up in London (UK) at the Institute of Physics and the Royal Astronomical Society.</p> </li> <li> <p>May/2020 LIGO-Virgo Collaboration GW Open Data Workshop #3. Co-organizer, invited to write and coordinate the Parameter Estimation tutorials for 100 students (virtual).</p> </li> <li> <p>Nov/2018 OzGrav workshop: Towards O3. Lead organizer, 20 participants from the OzGrav inference program. A software development sprint.</p> </li> <li> <p>July/2018 OzGrav workshop: Introduction to Inference. Lead organiser, 33 international participants. Training in Bayesian inference and software development. Identifying new projects across the OzGrav nodes and themes..</p> </li> </ul>"},{"location":"science/#miscellaneous","title":"Miscellaneous","text":"<ul> <li> <p>Nature Astronomy Community, Behind the Paper: Understanding the rotational evolution of the Vela pulsar during the 2016 glitch</p> </li> <li> <p>Institute of Physics, Gravitational Physics Group 2015 newsletter</p> </li> <li> <p>M. Franchin et al. Current driven nucleation of domain walls in cylindrical nanowires (2011)</p> </li> </ul>"},{"location":"science/#selected-presentations","title":"Selected Presentations","text":"<ul> <li> <p>Seminar given at the Cardiff Gravity Exploration Institute Searching and characterizing compact binary coalescence signals: challenges and solutions in real data</p> </li> <li> <p>Invited colloqium at the IJCLab on Gravitatational Wave Astronomy: from interferometric strain to astrophysics</p> </li> <li> <p>A presentation I have given at a few schools on Inferring the properties of gravitational-wave signals using Bayesian Inference</p> </li> <li> <p>Invited to SEPNet workshop Equality, Diversity &amp; Inclusion \u2013 Revisiting the leaky pipeline \u2013 short-term contracts and career planning. My slides are available here.</p> </li> <li> <p>Banff workshop Detection and Analysis of Gravitational Waves in the era of Multi-Messenger Astronomy: From Mathematical Modelling to Machine Learning. You can find a recording here and my slides here.</p> </li> <li> <p>Journal club at CAMK in which I presented Flickering of the Vela pulsar.</p> </li> <li> <p>Seminar at Bar-Ilan University, Israel, January 2021: The deepening mystery of the Vela radio-pulsar glitch</p> </li> <li> <p>GR22/Amaldi13 meeting, Valencia, Spain, July 2019: Gravitational Wave Detection: A Fully Bayesian Approach (contributed)</p> </li> <li> <p>IPTA annual meeting, Pune, India, June 2019: Internal neutron-star physics from the 2016 Vela glitch (contributed, remote)</p> </li> <li> <p>Astrophysics Colloquium, University of Melbourne, October 2018: Astrophysical inference and transient gravitational wave astronomy (invited)</p> </li> <li> <p>ASA Annual Scientific Meeting, Melbourne, Australia, June 2018:   Multimessenger follow-up of continuous gravitational wave candidates (contributed)</p> </li> <li> <p>Australasian pulsar meeting, September 2018: Periodic modulations and a glitch in PSR B1828-11 (remote)</p> </li> <li> <p>Institute for Nuclear Theory Workshop INT-18-71W, Astro-Solids, Dense Matter,   and Gravitational Waves (April 16 - 20, 2018): Continuous wave parameter estimation and non-standard signal follow up (invited)</p> </li> <li> <p>11th Bonn workshop on Formation and Evolution of Neutron Stars, Bonn, Germany, December 2017:  Neutron stars as continuous gravitational wave emitters</p> </li> <li> <p>Annual NewCompStar Conference, Istanbul, Turkey, 2016:   Learning about neutron stars from pulsar precession observations (contributed, best student talk prize)</p> </li> <li> <p>Annual NewCompStar Conference, Budapest, Hungary 2015:   Comparing different models of pulsar timing   noise (contributed)</p> </li> <li> <p>BritGrav, Birmingham, UK, 2015:   Applying Bayesian data analysis to learn about periodic variability in   pulsars (contributed)</p> </li> <li> <p>BritGrav, Cambridge, UK, 2014: Gravitational wave searches from noisy neutron stars (contributed, runner up prize for best talk by IoP)</p> </li> </ul>"},{"location":"science/#press","title":"Press","text":"<ul> <li>Interview on Adelaide FiveAA to discuss GW190425, the second binary neutron star event observed by LIGO &amp; Virgo</li> <li>Press for Nature Astronomy article \"Rotational evolution of the Vela pulsar during the 2016 glitch\":<ul> <li>The Age: Patient astronomers crack the code of super-dense spinning stars</li> <li>CNET: Astronomers watched a neutron star 'glitch' and can't yet explain it</li> <li>The Register: Mysterious 'glitch' in neutron stars may be down to an itch under the body's surface</li> <li>ABC \"Your Afternoon\" Helen Shield interviews my excellent co-author Jim Palfreyman</li> <li>Phys.org Glitch in neutron star reveals its hidden secrets</li> <li>Astronomy: Astronomers catch a pulsar 'glitching,' offering insights into the strange stars</li> <li>Forbes: A Radio Glitch Reveals The Structure Of A Neutron Star</li> <li>Advocator: Neutron Star Anomaly Revealed More Details On These Mysterious Space Objects</li> <li>ZME Science: Peculiar pulsar slows down before \u2018glitching\u2019</li> <li>Futarism: A NEUTRON STAR \u201cGLITCHED\u201d \u2014 AND SCIENTISTS NOTICED SOMETHING AMAZING</li> <li>Sci-News: Glitch in Vela Pulsar Provides Unique Opportunity to Study Neutron Star\u2019s Interior</li> <li>Science Daily: Glitch in neutron star reveals its hidden secrets</li> <li>Space.com: Weird Star Slows Down Before 'Glitching,' and No One Knows Why</li> <li>Science Alert: Astronomers Just Got Closer to Unravelling The Mystery of 'Glitching' Pulsars</li> <li>Live Science: Maybe Neutron Stars 'Glitch Out' So Much Because They're Full of Soup</li> <li>IFLS: A Glitch In A Neutron Star Allowed Astronomers To \"Peek\" At Its Interior</li> <li>Spektrum: Wenn Neutronensterne aus dem Takt geraten</li> <li>Physics World: Pulsar glitch suggests superfluid layers lie within neutron star</li> <li>SciShow News: August 16 update</li> </ul> </li> </ul>"},{"location":"notes/latex/","title":"Hints and tips on working with Latex","text":"<p>I work with a lot of people who use latex day to day to write documents. I see a lot of the same errors happening over and over again and this is my attempt to try and get those fixed. Therefore, this is not an article about how to do all the fun exciting things in Latex (like build diagrams with TikZ), but instead how to avoid common issues and get the most out the tool. Note that this is written predominantly as a guide to students submitting BSc/MSci projects.</p>"},{"location":"notes/latex/#overleaf","title":"Overleaf","text":"<p>Obviously, Overleaf is great and a revolution. However, it has one feature that has lost many a student marks on their final report: by default it will Try to compile despite errors. As a result, new users often make mistakes which simply appear as a little red flag at the top. But, it still produces output. So no worries. However, that red flag is telling you your latex code is broken in a fundamental way. Overleaf is very clever and even tries to fix it. However, these fixes are usually mistaken and end up with your output containing nonsense. Please make sure that you fix errors in your OverLeaf.</p>"},{"location":"notes/latex/#the-grammar-of-equations","title":"The grammar of equations","text":"<p>Equations are part of sentances (see, e.g. this style guide). As such, they should be treated as part of the paragraph. For example, your latex source code may look like:</p> <pre><code>Newton's law of gravitation is\n$$ F = G\\frac{m_1 m_2}{r^2} \\, , $$\nwhere $m_1$ and $m_2$ are the mass of the two objects, $G$ is the gravitational constant, and $r$ is the separation between the masses.\n</code></pre> <p>A few notes on this:</p> <ol> <li>After the equation, we add <code>\\, ,</code>. The first part <code>\\,</code> is adding a space, the second is adding a comma.</li> <li>the <code>where</code> continues the sentance. Do not add a space between the equation and the next line otherwise latex will treat this as a new paragraph (usually ending up with an indentation).</li> </ol> <p>Alternatively, the equation may be the end of the sentance, in which case it would finish as a full stop like this: <pre><code>Given two masses $m_1$ and $m_2$ separated by $r$, Newton's law of gravitation can be expressed as\n$$ F = G\\frac{m_1 m_2}{r^2} \\, . $$\n</code></pre></p>"},{"location":"notes/latex/#text-mode-subscripts","title":"Text-mode subscripts","text":"<p>Often, we use subscripts to denote some property or other. For example, let's say you want to define the initial velocity as <code>$v_{init}$</code>. The <code>init</code> here will be in math mode (i.e. italics). Formally, it should be intepreted as <code>i</code> times <code>n</code> times <code>i</code> (etc), which is not what you mean. Therefore you should either:</p> <ol> <li>Use text mode <code>$v_\\text{init}$</code> (or equivalent)</li> <li>Use the subscript package</li> </ol>"},{"location":"notes/latex/#use-packages","title":"Use packages","text":"<p>There are a number of very useful packages that I highly recommend everyone use by default whenever they write a latex document. These are:</p> <ol> <li>acronym. Use this to ensure that you correctly introduce acronyms on their first usage and only use them once. Never handle acronyms manually, instead include <code>\\usepackage{acronym}</code>. Then define the acronym     <pre><code>\\newacro{GW}{Gravitational Wave}\n</code></pre>     Then whenever you want to use the acronym, inster <code>\\ac{GW}</code>. On the fist use this will resolve to <code>Gravitational Wave (GW)</code>.</li> <li>cleverref. Use this to avoid writing <code>In Fig.~\\ref{fig:X} we see..</code> (which is hard to keep consistent). Instead simply write <code>In \\cref{fig:X} we see</code>. You can set the choice of capitalization with an optioni on import: <code>\\usepackage[capitalise]{cleveref}</code></li> <li>hyperref. Use this to turn links on. Here is some setup to control the color:     <pre><code>\\usepackage{hyperref}% add hypertext capabilities\n\\hypersetup{\n    colorlinks,\n    linkcolor={red!50!black},\n    citecolor={blue!90!black},\n    urlcolor={blue!80!black}\n}\n</code></pre></li> </ol>"},{"location":"notes/latex/#use-macros","title":"Use macros","text":"<p>If you have a compicated peice of latex that you are using in multiple places, create a macro. For example, if I wanted to keep referring to the software <code>sklearn</code> but I needed to use <code>texttt</code> to match the journal style, I would define     <pre><code>\\newcommand{\\sklearn}{\\texttt{scikit-learn}}\n</code></pre> Then at any point in the text I could write `\\sklearn' to include the text. This way, should I need to change the style I only need to do so in one place.</p> <p>You can also do the samething for mathematical symbols. This is expecially useful if you keep changing your mind on which symbol to use: define it once and then you can change it whenever you like. If you want to use it in the text as well, you can use <code>\\ensuremath</code> like this: <pre><code>\\newcommand{\\pastro}{\\ensuremath{p_{\\rm astro}}\\xspace}\n</code></pre> This way you can use <code>\\pastro</code> in an equation or in the text directly.</p> <p>It is typically a good idea to group together all your macro's and acro's at the top of the document in a single block of definitions.</p>"},{"location":"notes/latex/#textual-and-parenthetic-citations","title":"Textual and parenthetic citations","text":"<p>There are two different types of citations, textual and parenthic. Please this, e.g. this guide. To understand the difference. Then, if your bibliography style allows it, use the write one by using <code>\\citep</code> for parenthetic and <code>\\citet</code> for textual (assuming you are using <code>natbib</code>).</p>"},{"location":"notes/mac_os_liblapack/","title":"Mac os liblapack","text":"<p>If you run into an error with something like</p> <pre><code>ImportError ... Library not loaded @rpath/liblapack.d.dylib\n</code></pre> <p>This can be resolved by installation through conda:</p> <pre><code>conda install -c conda-forge libcblas\n</code></pre>"},{"location":"notes/splitting_up_mrs/","title":"Splitting up a large Merge Request","text":"<p>It is common when developing a new feature on a branch to end up fixing a number of bugs or extending functionality elsewhere in the code. After you are done, however, the code reviewers may well ask that the bug fixes be separated into a separate merge request. This makes their life easier. They can review the bug fixes and get these merges into master quickly. Meanwhile, your new feature may need a more in-depth review. It would not be sensible to hold up fixing bugs based on the review of new code.</p> <p>In this note, I'll describe the method I use to break such large merge requests up. Advice on merge requests in general can be found here.</p>"},{"location":"notes/splitting_up_mrs/#figure-out-where-the-changes-are","title":"Figure out where the changes are","text":"<p>Let's say you are developing on <code>feature-branch</code> and want to merge into <code>main-branch</code>. First, let's figure out which files have been modified:</p> <pre><code>$ git diff --name-only main-branch feature-branch\nsrc/utils.py\nsrc/magic.py\n</code></pre> <p>Here we can see that both <code>utils.py</code> and <code>magic.py</code> have been modified. Let's say that the changes to <code>utils.py</code> where all bug fixes while <code>magic.py</code> is a new file introducing new features. The code reviewers want to split up our merge request into two: one which only adds the bug fixes to <code>utils.py</code> and a second one which adds the new functionality in <code>magic.py</code>.</p>"},{"location":"notes/splitting_up_mrs/#the-problem-and-solution","title":"The Problem and Solution","text":"<p>The problem, from our perspective, is that the changes to <code>magic.py</code> depend on the changes to <code>utils.py</code>! Of course, we fixed the bugs so that it would run.</p> <p>To satisfy the code reviewers while not breaking our branch, we need to:</p> <ol> <li>Create a new merge request with only the changes to <code>utils.py</code> included</li> <li>Request the new merge request be merged into master</li> <li>Rebase our <code>feature-branch</code> to master (bringing with it the changes to <code>utils.py</code>)</li> <li>Now we have a <code>feature-branch</code> with only <code>magic.py</code> being changed, but all the while <code>feature-branch</code> never got broken.</li> </ol>"},{"location":"notes/splitting_up_mrs/#technical-how-to","title":"Technical how-to","text":"<p>The steps above can be done as follows:</p> <p>Create a new merge request with only the changes to <code>utils.py</code> included <pre><code>$ (main-branch) git checkout main-branch\n$ (main-branch) git checkout -b fix-utils  # Create branch of main-branch\n$ (fix-utils) git diff main-branch feature-branch -- src/utils.py &gt; utils_changes.patch  # Write the diff between the main and feature branches to a patch\n$ (fix-utils) git apply utils_changes.patch  # Apply the patch\n$ (fix-utils) git commit -m \"Fixing utils\"\n</code></pre> Now you can push this branch and create a merge request.</p> <p>Once <code>fix-utils</code> gets merged into <code>main-branch</code>, all you need to do is rebase your <code>feature-branch</code> to <code>main-branch</code> and then your merge request will only contain changes to <code>magic.py</code>. Magic.</p>"},{"location":"notes/waveform_projection/","title":"Waveform projection","text":"<pre><code>!pip install bilby lalsuite\n</code></pre> <pre><code>import lalsimulation as lalsim\nimport lal\nimport bilby\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.interpolate import interp1d\n\nsolar_mass = bilby.core.utils.constants.solar_mass\nparsec = bilby.core.utils.constants.parsec\n</code></pre>"},{"location":"notes/waveform_projection/#generating-time-domain-waveforms-and-projecting-them-onto-the-detectors","title":"Generating time domain waveforms and projecting them onto the detectors","text":"<p>In this notebook, I'll demonstrate how to use <code>lalsimulation</code> to generate a time-domain waveform and how to then use <code>bilby</code> to calculate the response of an interferometer.</p>"},{"location":"notes/waveform_projection/#setting-a-few-things-up","title":"Setting a few things up","text":"<p>To get started, let's define some properties of the data we want to simulate. This is the sampling frequency, duration and the amount of time before and after the trigger time (roughly speaking, the peak of the 2,2 mode).</p> <pre><code>sampling_frequency = 4096\ndeltaT = 1 / sampling_frequency\nduration = 4\npost_trigger_duration = 0.5\npre_trigger_duration = duration - post_trigger_duration\n</code></pre>"},{"location":"notes/waveform_projection/#generate-a-waveform","title":"Generate a waveform","text":"<p>Now, we'll use <code>lalsim.SimInspiralChooseTDWaveform</code> to generate the plus and cross polarization</p> <pre><code># Define the parameters in SI units\nmass_1 = 30 * solar_mass\nmass_2 = 30 * solar_mass\nspin_1x, spin_1y, spin_1z = 0, 0, 0\nspin_2x, spin_2y, spin_2z = 0, 0, 0\nluminosity_distance = 2000\ntheta_jn = 0\nphase = 0\n\nlongAscNodes = 0\neccentricity = 0\nmeanPerAno = 0\nLALParams = lal.CreateDict()\n\n# Get the approximant number from the name\nwaveform_approximant = \"IMRPhenomT\"\napproximant = lalsim.GetApproximantFromString(waveform_approximant)\n\n# Estimate a minimum frequency required to ensure the waveform covers the data\n# Note the 0.95 is a fudge factor as SimInspiralChirpStartFrequencyBound includes\n# only the leading order Newtonian coefficient.\nf_min = 0.95 * lalsim.SimInspiralChirpStartFrequencyBound(pre_trigger_duration, mass_1, mass_2)\nif lalsim.SimInspiralGetSpinFreqFromApproximant(approximant) == lalsim.SIM_INSPIRAL_SPINS_FLOW:\n    f_ref = f_min\nelse:\n    f_ref = 20\n\nh_plus_timeseries, h_cross_timeseries = lalsim.SimInspiralChooseTDWaveform(\n    mass_1, mass_2, spin_1x, spin_1y, spin_1z, spin_2x, spin_2y, spin_2z, luminosity_distance, theta_jn, phase, \n    longAscNodes, eccentricity, meanPerAno, deltaT, f_min, f_ref, LALParams,\n    approximant\n)\n</code></pre>"},{"location":"notes/waveform_projection/#extract-the-data","title":"Extract the data","text":"<pre><code>h_plus = h_plus_timeseries.data.data\nh_cross = h_cross_timeseries.data.data\nh_plus_time = np.arange(len(h_plus)) * h_plus_timeseries.deltaT + float(h_plus_timeseries.epoch)\nh_cross_time = np.arange(len(h_cross)) * h_cross_timeseries.deltaT + float(h_cross_timeseries.epoch)\n</code></pre>"},{"location":"notes/waveform_projection/#project-onto-the-hanford-interferometer","title":"Project onto the Hanford interferometer","text":"<pre><code>ra = 1.2\ndec = -3.1\ngeocent_time = 1126259462.1\npsi = 0.5\nH1 = bilby.gw.detector.get_empty_interferometer(\"H1\")\n\nplus_polarization_tensor = bilby.gw.utils.get_polarization_tensor(ra, dec, geocent_time, psi, \"plus\")\nf_plus = np.einsum('ij,ij-&gt;', H1.detector_tensor, plus_polarization_tensor)\n\ncross_polarization_tensor = bilby.gw.utils.get_polarization_tensor(ra, dec, geocent_time, psi, \"cross\")\nf_cross = np.einsum('ij,ij-&gt;', H1.detector_tensor, cross_polarization_tensor)\n\nstrain = f_plus * h_plus + f_cross * h_cross\nstrain_time = h_plus_time\n</code></pre>"},{"location":"notes/waveform_projection/#plot-the-data","title":"Plot the data","text":"<pre><code>plt.plot(strain_time, strain)\nplt.ylabel(\"Strain\")\nplt.xlabel(f\"GPS time [s]\")\nplt.show()\n</code></pre> <p>From the plot above (or by inspecting <code>strain</code> and <code>strain_time</code>), we see that <code>SimInspiralChooseTDWaveform</code> outputs the strain on a grid of times with sampling frequency <code>1/deltaT</code>, but that the duration is determined by <code>f_min</code> and that peak of the 2,2 mode occurs at <code>0</code>.</p> <p>We can translate this to the time measured by a detector by simply adding <code>geocent_time</code>, e.g.</p> <pre><code>strain_detector_time = strain_time + geocent_time\n</code></pre> <p>But, we'll want to compare our predicted strain with a timeseries of detector data which will be sampled on a different grid (even if the sampling frequency is identical, we would not expect a sampled timeseries to align with the peak of the 2,2 mode!). To convert, we can interpolate.</p>"},{"location":"notes/waveform_projection/#interpolate-onto-a-sampled-data-grid","title":"Interpolate onto a sampled data grid","text":"<pre><code>n = sampling_frequency * duration\ndata_start_time = int(geocent_time) - pre_trigger_duration\ndata_detector_time = np.arange(n) / sampling_frequency + data_start_time \nh_interp = interp1d(strain_detector_time, strain, fill_value=0, bounds_error=False)(data_detector_time)\n</code></pre> <pre><code>fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15, 6))\nax1.plot(data_detector_time - geocent_time, h_interp)\nax1.plot(strain_detector_time - geocent_time, strain, \"--\")\nax1.set_ylabel(\"Strain\")\nax1.set_xlabel(f\"GPS time - {geocent_time} [s]\")\nax2.plot(data_detector_time - geocent_time, h_interp, label=\"Interpolated\")\nax2.plot(strain_detector_time - geocent_time, strain, \"--\", label=\"Strain\")\nax2.set_ylabel(\"Strain\")\nax2.set_xlabel(f\"GPS time - {geocent_time} [s]\")\nax2.set_xlim(-0.1, 0.1)\nax2.legend()\nplt.show()\n</code></pre>"},{"location":"notes/waveform_projection/#putting-it-all-together-into-a-single-function","title":"Putting it all together into a single function","text":"<pre><code>from bilby.gw.utils import _get_lalsim_approximant, convert_args_list_to_float\n\ndef get_gw_waveform(time, parameters, waveform_approximant, reference_frequency, bilby_detector, fudge=0.95, reference_frame=None, pre_trigger_duration=None, error=False):\n    par, _ = bilby.gw.conversion.convert_to_lal_binary_black_hole_parameters(parameters)\n\n    mass_1_SI = par[\"mass_1\"] * solar_mass\n    mass_2_SI = par[\"mass_2\"] * solar_mass\n    luminosity_distance_SI = par[\"luminosity_distance\"] * 1e6 * parsec\n\n    # Extract information about the time series\n    deltaT = time[1] - time[0]\n    if pre_trigger_duration is None:\n        nearest_trigger_idx = np.argmin(np.abs(time - par[\"geocent_time\"]))\n        pre_trigger_duration = time[nearest_trigger_idx] - time[0]\n\n    # Get the approximant number from the name\n    approximant = _get_lalsim_approximant(waveform_approximant)\n\n    # Estimate a minimum frequency required to ensure the waveform covers the data\n    # Note the 0.95 is a fudge factor as SimInspiralChirpStartFrequencyBound includes\n    # only the leading order Newtonian coefficient.\n    f_min = fudge * lalsim.SimInspiralChirpStartFrequencyBound(\n        pre_trigger_duration, \n        mass_1_SI,\n        mass_2_SI,\n    )\n\n    # Check if the reference frequency is used, if not use f_min\n    if lalsim.SimInspiralGetSpinFreqFromApproximant(approximant) == lalsim.SIM_INSPIRAL_SPINS_FLOW:\n        f_ref = f_min\n    elif reference_frequency == \"fmin\":\n        f_ref = f_min\n    else:\n        f_ref = reference_frequency\n\n    iota, spin_1x, spin_1y, spin_1z, spin_2x, spin_2y, spin_2z = bilby.gw.conversion.bilby_to_lalsimulation_spins(\n        theta_jn=par[\"theta_jn\"], phi_jl=par[\"phi_jl\"], tilt_1=par[\"tilt_1\"], tilt_2=par[\"tilt_2\"],\n        phi_12=par[\"phi_12\"], a_1=par[\"a_1\"], a_2=par[\"a_2\"], mass_1=mass_1_SI, mass_2=mass_2_SI,\n        reference_frequency=f_ref, phase=par[\"phase\"])\n\n    if \"zenith\" in par and \"azimuth\" in par:\n        par[\"ra\"], par[\"dec\"] = bilby.gw.utils.zenith_azimuth_to_ra_dec(\n            par['zenith'], par['azimuth'], par[\"geocent_time\"], reference_frame)\n\n    longitude_ascending_nodes = 0.\n    eccentricity = 0.\n    mean_per_ano = 0.\n    waveform_dictionary = lal.CreateDict()\n\n    args = convert_args_list_to_float(\n        mass_1_SI, mass_2_SI,\n        spin_1x, spin_1y, spin_1z,\n        spin_2x, spin_2y, spin_2z,\n        luminosity_distance_SI, iota, par[\"phase\"],\n        longitude_ascending_nodes, eccentricity, mean_per_ano,\n        deltaT, f_min, f_ref)\n\n    h_plus_timeseries, h_cross_timeseries = lalsim.SimInspiralChooseTDWaveform(\n        *args, waveform_dictionary, approximant   \n    )\n\n    plus_polarization_tensor = bilby.gw.utils.get_polarization_tensor(par[\"ra\"], par[\"dec\"], par[\"geocent_time\"], par[\"psi\"], \"plus\")\n    f_plus = np.einsum('ij,ij-&gt;', bilby_detector.detector_tensor, plus_polarization_tensor)\n\n    cross_polarization_tensor = bilby.gw.utils.get_polarization_tensor(par[\"ra\"], par[\"dec\"], par[\"geocent_time\"], par[\"psi\"], \"cross\")\n    f_cross = np.einsum('ij,ij-&gt;', bilby_detector.detector_tensor, cross_polarization_tensor)\n\n    h_plus = h_plus_timeseries.data.data\n    h_cross = h_cross_timeseries.data.data\n    h_plus_time = np.arange(len(h_plus)) * h_plus_timeseries.deltaT + float(h_plus_timeseries.epoch)\n\n    h = f_plus * h_plus + f_cross * h_cross\n    t = h_plus_time + par[\"geocent_time\"]\n\n    h_interp = interp1d(t, h, fill_value=0, bounds_error=False)(time)\n\n    if h_interp[0] == 0:\n        msg = \"Generated waveform was too short\"\n        if error:\n            raise ValueError(msg)\n        else:\n            print(msg)\n    return h_interp\n</code></pre> <pre><code>parameters = dict(\n    mass_1=36., mass_2=29., chi_1=0.4, chi_2=0.3, luminosity_distance=2000., theta_jn=0.4, psi=2.659,\n    phase=2.8, geocent_time=geocent_time, ra=1.375, dec=-1.2108\n)\n\nfor waveform in [\"IMRPhenomT\", \"SEOBNRv4T\", \"TEOBResumS\"]:\n    w = get_gw_waveform(data_detector_time, parameters, waveform, \"fmin\", H1)\n    plt.plot(data_detector_time - geocent_time, w, label=waveform)\nplt.xlim(-0.1, 0.05)\nplt.ylabel(\"Strain\")\nplt.xlabel(f\"GPS time - {geocent_time} [s]\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"notes/waveform_projection/#time-the-generation","title":"Time the generation","text":"<pre><code>prior = bilby.gw.prior.BBHPriorDict()\nprior[\"geocent_time\"] = bilby.core.prior.Uniform(geocent_time - 0.1, geocent_time + 0.1)\n</code></pre> <pre><code>10:29 bilby INFO    : No prior given, using default BBH priors in /home/greg/bilby/bilby/gw/prior_files/precessing_spins_bbh.prior.\n</code></pre> <pre><code>%%timeit\n_ = get_gw_waveform(data_detector_time, prior.sample(), \"SEOBNRv4P\", 20, H1, fudge=0.8)\n</code></pre> <pre><code>1.28 s \u00b1 96.5 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</code></pre> <pre><code>%%timeit\n_ = get_gw_waveform(data_detector_time, prior.sample(), \"IMRPhenomTP\", 20, H1, fudge=0.8)\n</code></pre> <pre><code>30.9 ms \u00b1 2.82 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</code></pre> <pre><code>%%timeit\n_ = get_gw_waveform(data_detector_time, prior.sample(), \"IMRPhenomPv2\", 20, H1, fudge=0.8, pre_trigger_duration=pre_trigger_duration)\n</code></pre> <pre><code>13.6 ms \u00b1 237 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</code></pre>"},{"location":"notes/waveform_projection/#profile-the-generation","title":"Profile the generation","text":"<pre><code>%load_ext line_profiler\n</code></pre> <pre><code>%lprun -T profile.txt -s -u 1e-3 -f get_gw_waveform get_gw_waveform(data_detector_time, prior.sample(), \"IMRPhenomTP\", 20, H1, fudge=0.85)\n</code></pre> <pre><code>*** Profile printout saved to text file 'profile.txt'.\n</code></pre> <pre><code>!head -n 15 profile.txt\n</code></pre> <pre><code>Timer unit: 0.001 s\n\nTotal time: 0.029128 s\nFile: /tmp/ipykernel_1170/3049955836.py\nFunction: get_gw_waveform at line 3\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n     3                                           def get_gw_waveform(time, parameters, waveform_approximant, reference_frequency, bilby_detector, fudge=0.95, reference_frame=None, pre_trigger_duration=None, error=False):\n     4         1          0.1      0.1      0.2      par, _ = bilby.gw.conversion.convert_to_lal_binary_black_hole_parameters(parameters)\n     5                                                   \n     6         1          0.0      0.0      0.0      mass_1_SI = par[\"mass_1\"] * solar_mass\n     7         1          0.0      0.0      0.0      mass_2_SI = par[\"mass_2\"] * solar_mass\n     8         1          0.0      0.0      0.0      luminosity_distance_SI = par[\"luminosity_distance\"] * 1e6 * parsec\n     9\n</code></pre> <pre><code>!tail -n +9 profile.txt | sort -nr -k 3 | head -n 10\n</code></pre> <pre><code>    60         4         27.6      6.9     94.7      h_plus_timeseries, h_cross_timeseries = lalsim.SimInspiralChooseTDWaveform(\n    77         1          0.6      0.6      2.0      h_interp = interp1d(t, h, fill_value=0, bounds_error=False)(time)\n    64         1          0.3      0.3      0.9      plus_polarization_tensor = bilby.gw.utils.get_polarization_tensor(par[\"ra\"], par[\"dec\"], par[\"geocent_time\"], par[\"psi\"], \"plus\")\n    38         2          0.2      0.1      0.7      iota, spin_1x, spin_1y, spin_1z, spin_2x, spin_2y, spin_2z = bilby.gw.conversion.bilby_to_lalsimulation_spins(\n    74         1          0.1      0.1      0.3      h = f_plus * h_plus + f_cross * h_cross\n    72         1          0.1      0.1      0.3      h_plus_time = np.arange(len(h_plus)) * h_plus_timeseries.deltaT + float(h_plus_timeseries.epoch)\n    67         1          0.1      0.1      0.2      cross_polarization_tensor = bilby.gw.utils.get_polarization_tensor(par[\"ra\"], par[\"dec\"], par[\"geocent_time\"], par[\"psi\"], \"cross\")\n    13         1          0.1      0.1      0.4          nearest_trigger_idx = np.argmin(np.abs(time - par[\"geocent_time\"]))\n     4         1          0.1      0.1      0.2      par, _ = bilby.gw.conversion.convert_to_lal_binary_black_hole_parameters(parameters)\n    85         1          0.0      0.0      0.0      return h_interp\n</code></pre> <p>We can remove that lookup for the <code>pre_trigger_duration</code> if we know it</p> <pre><code>%lprun -T profile.txt -s -u 1e-3 -f get_gw_waveform get_gw_waveform(data_detector_time, prior.sample(), \"IMRPhenomTP\", 20, H1, fudge=0.85, pre_trigger_duration=pre_trigger_duration)\n!tail -n +9 profile.txt | sort -nr -k 3 | head -n 10\n</code></pre> <pre><code>*** Profile printout saved to text file 'profile.txt'. \n    60         4         38.3      9.6     91.5      h_plus_timeseries, h_cross_timeseries = lalsim.SimInspiralChooseTDWaveform(\n    77         1          1.5      1.5      3.7      h_interp = interp1d(t, h, fill_value=0, bounds_error=False)(time)\n    74         1          0.8      0.8      2.0      h = f_plus * h_plus + f_cross * h_cross\n    38         2          0.4      0.2      0.9      iota, spin_1x, spin_1y, spin_1z, spin_2x, spin_2y, spin_2z = bilby.gw.conversion.bilby_to_lalsimulation_spins(\n    64         1          0.2      0.2      0.6      plus_polarization_tensor = bilby.gw.utils.get_polarization_tensor(par[\"ra\"], par[\"dec\"], par[\"geocent_time\"], par[\"psi\"], \"plus\")\n     4         1          0.2      0.2      0.4      par, _ = bilby.gw.conversion.convert_to_lal_binary_black_hole_parameters(parameters)\n    75         1          0.1      0.1      0.1      t = h_plus_time + par[\"geocent_time\"]\n    72         1          0.1      0.1      0.3      h_plus_time = np.arange(len(h_plus)) * h_plus_timeseries.deltaT + float(h_plus_timeseries.epoch)\n    85         1          0.0      0.0      0.0      return h_interp\n    84                                                       print(msg)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"old_notes/importance_reweighting_example/","title":"Importance reweighting example","text":"<pre><code>   \"\"\"\n   Script to test importance reweighting\n\n\n   In this script we generate data according to\n\n     y = m x + c + Normal(0, sigma)\n\n   Then\n\n   1) Calculate the \"full\" posterior P(m, c | data)\n   2) Calculate the \"partial\" posterior P(m | data, c=0) (note c=0 in the injection)\n   3) Use importance reweighting to calculate P(m| data) from the \"partial\" results\n\n   \"\"\"\n   import matplotlib.pyplot as plt\n   import numpy as np\n   import bilby\n   from scipy.special import logsumexp\n   import tqdm\n\n   np.random.seed(1234)\n   outdir = '.'\n   sampler = 'dynesty'\n   npoints = 5000\n\n\n   def marginalized_log_likelihood_over_c(m, likelihood):\n       \"\"\" Calculates L(data| m), marginalized over c\n\n       Parameters\n       ----------\n       m: float\n           The fixed value of m at which to calculate the marginalized likelihood\n       likelihood: bilby.core.likelihood.Likelihood instance\n           Used to evaluate the log likelihood\n\n       Note\n       ----\n           Integration range chosen to cover the region of interest. Note, this\n           neglects the normalization factors which don't count in the weight\n           calculation.\n       \"\"\"\n       likelihood.parameters['m'] = m\n       c_array = np.linspace(-0.2, 0.2, 100)\n       integrand = []\n       for c in c_array:\n           likelihood.parameters['c'] = c\n           integrand.append(likelihood.log_likelihood())\n       return logsumexp(integrand)\n\n\n   def model(x, m, c):\n       return m * x + c\n\n\n   # Injection parameters and create data\n   m = 1\n   c = 0\n   sigma = 0.1\n   N = 100\n   x = np.linspace(0, 1, N)\n   y = model(x, m, c) + np.random.normal(0, sigma, N)\n\n   likelihood = bilby.core.likelihood.GaussianLikelihood(x, y, model)\n\n   # Run the full PE\n   priors = dict()\n   priors['m'] = bilby.core.prior.Uniform(0, 5, 'm')\n   priors['c'] = bilby.core.prior.Uniform(-2, 2, 'c')\n   priors['sigma'] = sigma\n   full_result = bilby.run_sampler(\n       likelihood=likelihood, priors=priors, sampler=sampler, npoints=npoints,\n       outdir=outdir, label='full')\n   full_result.plot_corner()\n\n   # Run the constrained PE\n   priors = dict()\n   priors['m'] = bilby.core.prior.Uniform(0, 5, 'm')\n   priors['c'] = 0\n   priors['sigma'] = sigma\n   partial_result = bilby.run_sampler(\n       likelihood=likelihood, priors=priors, sampler=sampler, npoints=npoints,\n       outdir=outdir, label='partial')\n   partial_result.plot_corner()\n\n   # Pull out the uniformly-weighted samples from the full and partial runs\n   full_m_samples = full_result.posterior.m.values\n   partial_m_samples = partial_result.posterior.m.values\n\n   # Calculate primed likelihood\n   log_likelihood_prime = []\n   for m in tqdm.tqdm(partial_m_samples):\n       log_likelihood_prime.append(marginalized_log_likelihood_over_c(m, likelihood))\n   log_likelihood_prime = np.array(log_likelihood_prime)\n\n   # Calculate p, the normalized probably for each sample in partial_m_samples\n   log_likelihood = partial_result.posterior.log_likelihood.values\n   weights = log_likelihood_prime - log_likelihood\n   p = np.exp(weights)\n   p /= np.sum(p)\n\n   # Reweight to get corrected samples\n   reweight_samples = np.random.choice(partial_m_samples, size=30000, p=p)\n\n   # Plot\n   bins = np.linspace(0.9, 1.1, 50)\n   fig, ax = plt.subplots()\n   ax.hist(full_m_samples, bins=bins, density=True, label=\"full\",\n           histtype='step', linewidth=2.5)\n   ax.hist(partial_m_samples, bins=bins, density=True, alpha=0.5, label=\"partial\")\n   ax.hist(reweight_samples, bins=bins, density=True, alpha=0.5, label=\"resampled\")\n   ax.legend()\n   ax.set_xlabel(\"m\")\n   plt.savefig(\"posterior\")\n</code></pre> <p>Having run this script, we obtain three images. First, the full posterior</p> <p></p> <p>Second, the posterior when fixing <code>c=0</code></p> <p></p> <p>Finally, the rewighted posterior from fixed case</p> <p></p>"}]}